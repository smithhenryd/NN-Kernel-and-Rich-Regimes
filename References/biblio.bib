@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={7413--7424},
  year={2019}
}

@article{chizat2018lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}
@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  volume={8},
  year={2018}
}
@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}
@inproceedings{gunasekar2018implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}
@article{gunasekar2018implicitconv,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}
@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}
@inproceedings{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
}
@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}
@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason and Liu, Qiang and Ma, Tengyu},
  year={2019}
}
@misc{wibisono2016,
   author={Wibisono, Andre},
   title={Gradient flow and gradient descent},
   editor={GitHub Pages},
   month={July},
   year={2016},
   howpublished = "\url{http://awibisono.github.io/2016/06/13/gradient-flow-gradient-descent.html}"
 }
@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}