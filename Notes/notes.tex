\documentclass{article}

\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% Formatting and images 
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{soul}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{csquotes}

%% Packages for mathematical typesetting
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{pgf}
\usepackage{comment}
\usepackage{float}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{bbm}

\newtheorem{manualtheoreminner}{Theorem}
\newenvironment{manualtheorem}[1]{%
  \renewcommand\themanualtheoreminner{#1}%
  \manualtheoreminner
}{\endmanualtheoreminner}

\newtheorem{manuallemmainner}{Lemma}
\newenvironment{manuallemma}[1]{%
  \renewcommand\themanuallemmainner{#1}%
  \manuallemmainner
}{\endmanualtheoreminner}

\newtheorem{manualcorollaryinner}{Corollary}
\newenvironment{manualcorollary}[1]{%
  \renewcommand\themanualcorollaryinner{#1}%
  \manualcorollaryinner
}{\endmanualcorollaryinner}

\newtheorem*{conjecture}{Conjecture}

% Title content
\title{\textbf{Notes on Kernel and Rich Limits\\ in Neural Network Training}}
\author[]{Henry Smith}
\affil[]{\normalsize Yale University}
\date{\today}

\begin{document}

\maketitle
\section{Kernel and Rich Regimes in Overparametrized Models, Woodworth et al. 2020}

\subsection{Problem Setup}
The paper considers models $f: \mathbb{R}^p \times \mathcal{X} \rightarrow \mathbb{R}$ which map parameters $\boldsymbol{w} \in \mathbb{R}^p$ and observations $\boldsymbol{x} \in \mathcal{X}$ to predictions $f(\boldsymbol{w}, \boldsymbol{x})$. Let $F(\boldsymbol{w}) \in \{f: \mathcal{X} \rightarrow \mathbb{R} \}$ be the predictor implemented by the parameters $\boldsymbol{w}$. Of particular interest to the authors are models which are linear in the observations $\boldsymbol{x}$ (but not necessarily in the parameters $\boldsymbol{w}$). Since, in this case, $F(\boldsymbol{w})$ is in the dual space of $\mathcal{X}$, then we have that by Riesz Representation, $F(\boldsymbol{w})(\boldsymbol{x}) = \langle\boldsymbol{\beta}_{\boldsymbol{w}}, \boldsymbol{x} \rangle$ for some $\boldsymbol{\beta}_{\boldsymbol{w}}$. The paper focuses on $D$-homogeneous models in the parameter space $\boldsymbol{w}$ as discussed in \cite{chizat2018lazy}, which satisfy $F(c \cdot \boldsymbol{w}) = c^D F(\boldsymbol{w})$ for any $c \in \mathbb{R}_+$. For $D$-homogeneous networks, it is important to note that scaling the output by a factor of $c > 0$, which is the focus of \cite{chizat2018lazy}, is equivalent to scaling the input by a factor of $c^{1/D}$. 

Further, the paper considers the square loss function $L(\boldsymbol{w}) = \widetilde{L}(F(\boldsymbol{w})) = \sum_{i=1}^N(f(\boldsymbol{w}, \boldsymbol{x}_n) - y_n)^2$ for the model $F$ over the training set $(\boldsymbol{x}_1, y_1), \ldots, (\boldsymbol{x}_N, y_N).$ Woodworth and colleagues minimize the loss $L$ using the gradient flow dynamics, which can be thought of as gradient descent with the stepsize $\eta$ limiting to 0. More explicitly, the gradient flow dynamics are
\begin{align*}
    \dot{\boldsymbol{w}}(t) = - \nabla L(\boldsymbol{w}(t)).
\end{align*}

The principal result of \cite{chizat2018lazy} is that, under suitable conditions, the gradient flow dynamics on $\boldsymbol{w}$ approach those of a linearized model as the scale of initialization approaches infinity. To capture the scale of initialization, the authors consider the parameter $\alpha \in \mathbb{R}_+$. For fixed initialization scale $\alpha$, let $\boldsymbol{w}_{\alpha, \boldsymbol{w}_0}(t)$ be the gradient flow path with the initial condition $\boldsymbol{w}_{\alpha, \boldsymbol{w}_0}(0) = \alpha \boldsymbol{w}(0)$.

Woodworth and colleagues are particularly interested in the case of  $N \ll p$, where there are many global minimizers of $L(\boldsymbol{w})$ with $L(\boldsymbol{w}) = 0$. That is, the model is overparameterized/underdetermined and so we can fit the observations exactly. Of particular interest to the paper is which of the global minimizers $\boldsymbol{w}_{\alpha, \boldsymbol{w}_0}^{\infty} := \lim_{t \rightarrow \infty} \boldsymbol{w}_{\alpha, \boldsymbol{w}_0}(t)$ gradient flow converges to. 

\subsection{Kernel Regime}
Locally, the gradient descent depends only on the first-order approximation in $\boldsymbol{w}$:
\begin{align*}
    f(\boldsymbol{w}, \boldsymbol{x}) = f(\boldsymbol{w} - \boldsymbol{w}(t), \boldsymbol{x}) + \langle \boldsymbol{w} - \boldsymbol{w}(t), \phi_{\boldsymbol{w}(t)}(\boldsymbol{x}) \rangle + \mathcal{O}( \left\Vert \boldsymbol{w} - \boldsymbol{w}(t) \right\Vert^2),
\end{align*}
where $\phi_{\boldsymbol{w}(t)}(\boldsymbol{x}) = \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t), \boldsymbol{x})$ is the \textit{feature map} corresponding to the \textit{tangent kernel} $K_{\boldsymbol{w}(t)}(\boldsymbol{x}, \boldsymbol{x}') = \langle \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t), \boldsymbol{x}), \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t), \boldsymbol{x}')\rangle$. Clearly this approximation is linear in $\boldsymbol{w}$ but need not be linear in $\boldsymbol{x}$ if the underlying model $F(\boldsymbol{w})$ is not linear in $\boldsymbol{x}$.

\cite{chizat2018lazy} provides an in-depth look at the \textit{kernel regime} in which the gradient flow dynamics $\dot{w}(t)$ \textit{depend only on a linear function in $\boldsymbol{w}$}. This means that $\phi_{\boldsymbol{w}(t)}(\boldsymbol{x})$, and thus the tangent kernel, is constant throughout training. Under certain conditions on the $D$-homogeneous model $F(\boldsymbol{w})$ and the loss function $L$, the kernel regime manifests as $\alpha \rightarrow \infty$ in the initialization $\boldsymbol{w}_{\alpha}(0) = \alpha \boldsymbol{w}(0)$. Since this is a theoretical limit, we use the kernel regime to refer to the case in which the gradient flow dynamics are \enquote{close to} the linearized dynamics and so the \textit{tangent kernel does not change significantly} throughout training (see Theorem 2.4 in \cite{chizat2018lazy}).

In particular, under the kernel regime, training $f(\boldsymbol{w}, \boldsymbol{x})$ is equivalent to training the affine model $f_K(\boldsymbol{w}, \boldsymbol{x}) = \alpha^Df(\boldsymbol{w}(0), \boldsymbol{x}) + \langle \phi_{\boldsymbol{w}(0)}(\boldsymbol{x}), \boldsymbol{w} - \boldsymbol{w}(0) \rangle$ using kernelized gradient descent/flow with the kernel $K_{\boldsymbol{w}_0}$ and a bias term $f(\boldsymbol{w}_0, \boldsymbol{x})$. Observe that $\phi_{\boldsymbol{w}(t)}(\boldsymbol{x}) = \nabla_{\boldsymbol{w}(t)} f_K(\boldsymbol{w}, \boldsymbol{x}) = \phi_{\boldsymbol{w}(0)}(\boldsymbol{x})$, and so the tangent kernel is indeed constant throughout training. Minimizing the loss of this affine model with gradient flow reaches the solution nearest to initialization where distance is measured with respect to the RKHS norm determined by $K_0$. That is, $F(\boldsymbol{w}_{\alpha}^{\infty}) = \text{argmin}_h \left\Vert h - F(\apha \boldsymbol{w}_0) \right\Vert_{K_0} \ \text{s.t.} \ h(X) = \boldsymbol{y}$. Here, our RKHS norm is $\left\Vert g \right\Vert_{K_0} = \inf \{ \left\Vert \ell \right\Vert_2: \ell \in \mathbb{R}^p, g(\boldsymbol{x}) = \langle \ell, \phi_{\boldsymbol{w}(0)}(\boldsymbol{x}) \rangle, \ \forall \boldsymbol{x} \in \mathcal{X}\}$. \cite{chizat2018lazy} advises choosing an unbiased initialization $\boldsymbol{w}_0$ such that $F(\boldsymbol{w}_0) = 0$. This means that the bias term $\alpha^Df(\boldsymbol{w}(0), \boldsymbol{x})$ depending on the initialization scale vanishes. 

\subsection{Example: Linear Regression}
The primary contribution of the paper is the explicit characterization of the implicit bias when training with gradient descent as a function of $\alpha$, the scale of initialization, in the following least-squares problem:
For $\mathcal{X} = \mathbb{R}^d$, define the model
\begin{align*}
    f(\boldsymbol{w}, \boldsymbol{x}) = \sum_{i=1}^d (\boldsymbol{w}_{+, i}^2 - \boldsymbol{w}_{-, i}^2)\boldsymbol{x}_i = \langle \boldsymbol{\beta}_{\boldsymbol{w}}, \boldsymbol{x} \rangle, \quad \boldsymbol{w} = \begin{bmatrix}
                        \boldsymbol{w}_+ \\
                        \boldsymbol{w}_-
                        \end{bmatrix} \in \mathbb{R}^{2d},
    \quad \boldsymbol{\beta}_{\boldsymbol{w}} = \boldsymbol{w}_+^2 - \boldsymbol{w}_-^2
\end{align*}
where $\boldsymbol{z}^2$ for $\boldsymbol{z} \in \mathbb{R}^d$ denotes element-wise squaring. This model represents a \enquote{diagonal} neural network where each input component $\boldsymbol{x}_i$ is connected to a positive input unit with weight $\boldsymbol{w}_{+, i}^2$ and a negative input unit with weight $\boldsymbol{w}_{-, i}^2$. This differs from the least-squares problem discussed in \cite{gunasekar2018implicit} where there is a single set of positive weights representing the squared least-squares coefficients (i.e. we have model $f(\boldsymbol{w}, \boldsymbol{x}) = \sum_{i=1}^d \boldsymbol{w}_i^2\boldsymbol{x}_i$). The authors prefer the former formulation of the least-squares problem for two reasons: (1) by choosing $\boldsymbol{w}_0$ such that $\boldsymbol{w}_+ = \boldsymbol{w}_-$, then the model vanishes at initialization $F(\alpha \boldsymbol{w}) = 0$ without this being a saddle point of the objective and (2) the image of $F(\boldsymbol{w})$ is all linear functionals i.e. $\text{img}( F(\boldsymbol{w})) = \mathcal{X}^*$.

The authors study the underdetermined case $N \ll d$ when there are many solution vectors $\boldsymbol{\beta} \in \mathbb{R}^d$ that satisfy $X 
\boldsymbol{\beta} = \boldsymbol{y}$. Let $\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty}$ be the solution reached by gradient flow when initialized at $\boldsymbol{w}_+(0) = \boldsymbol{w}_-(0) = \alpha \boldsymbol{w}_0$. Woodworth and colleagues first consider the case of $\boldsymbol{w}_0 = \vec{1}$ and then generalize their results.

For $\boldsymbol{w}_0 = \vec{1}$, it is easy to verify that the tangent kernel at initialization is $K_{\boldsymbol{w}(0)}(\boldsymbol{x}, \boldsymbol{x}') = 8 \alpha^2 \langle \boldsymbol{x}, \boldsymbol{x}' \rangle$, which is simply a scaling of the standard inner product kernel (just write out $\nabla_{\boldsymbol{w}} f(\boldsymbol{w}, \boldsymbol{x}) |_{\boldsymbol{w} = \boldsymbol{w}(0)}$).
That is, we have $\left\Vert \boldsymbol{\beta} \right\Vert_{K_0} \propto \left\Vert \boldsymbol{\beta} \right\Vert_2$. And so in the kernel regime, $\boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}$ is equal to the minimum $\ell_2$ solution $\boldsymbol{\beta}_{\ell_2}^* := \text{argmin}_{X \boldsymbol{\beta} = y} \left\Vert \boldsymbol{\beta} \right\Vert_2$. Conversely, as $\alpha \rightarrow 0$, we approach what Woodworth and colleagues classify as the \enquote{rich limit.} Under the rich limit, the tangent kernel changes significantly during optimization, and the model $f(\boldsymbol{w}, \boldsymbol{x})$ is not linear in $\boldsymbol{w}$. For the linear regression problem under consideration with $\boldsymbol{w}_0 = \vec{1}$, \cite{gunasekar2018implicit} proves in Corollary \ref{gunasekharcor2} that $\lim_{\alpha \rightarrow 0} \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty} = \boldsymbol{\beta}_{\ell_1}^* = \text{argmin}_{X \boldsymbol{\beta} = y} \left\Vert \boldsymbol{\beta}\right\Vert_1$.

And so we understand that for the current model of interest with initialization $\boldsymbol{w}_0 = \vec{1}$, the gradient flow solution in the kernel regime $\alpha \rightarrow \infty$ is the minimum $\ell_2$ solution and the solution in the rich regime $\alpha \rightarrow 0$ is the minimum $\ell_1$ solution. 

\subsection{Interpolation Between the Rich and Kernel Regimes}
What the authors seek to understand, however, is how the gradient flow solution interpolates between the rich and kernel limits. That is, what can we say about the implicit bias when training the linear regression model for $\alpha$ small, for example? This is the subject of the following theorem:
\begin{manualtheorem}{1}\label{woodworththm1}
For any $0 < \alpha < \infty$, if the gradient flow solution $\boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}$ for the squared parameterization model satisfies $X\boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty} = \boldsymbol{y}$, then 
\begin{align*}
    \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty} = \underset{\boldsymbol{\beta}}{argmin} \ Q_{\alpha}(\boldsymbol{\beta}) \ \text{s.t.} \ X\boldsymbol{\beta} = \boldsymbol{y}
\end{align*}
where $Q_{\alpha}(\boldsymbol{\beta}) = \alpha^2 \sum_{i=1}^d q\left(\frac{\boldsymbol{\beta}_i}{\alpha^2}\right)$ and $q(z) = \int_0^z \text{arcsinh}\left(\frac{u}{2} \right) du = 2 - \sqrt{4+z^2} + z \text{arcsinh}\left(\frac{z}{2} \right).$
\end{manualtheorem}
See \cite{woodworth2020kernel} for further analysis of how $Q_{\alpha}(\boldsymbol{\beta})$ reproduces the $\ell_2$ and $\ell_1$ minimization problems as $\alpha \rightarrow \infty$ and $\alpha \rightarrow 0$, respectively. In Theorem \ref{woodworththm2}, Woodworth and colleagues quantify the scale of $\alpha$ necessary to guarantee approximation of the minimum $\ell_1$ and $\ell_2$ solutions:
\begin{manualtheorem}{2}\label{woodworththm2}
For any $0 < \epsilon < d$, under the setting of Theorem \ref{woodworththm1} with $\boldsymbol{w}_0 = \vec{1}$, 
\begin{align*}
    \alpha \leq \min \left\{ (2(1 + \epsilon)\left\Vert \boldsymbol{\beta}_{\ell_1}^* \right\Vert_1)^{-\frac{2+ \epsilon}{2\epsilon}}, \exp \left(-d/(\epsilon \left\Vert \boldsymbol{\beta}_{\ell_1}^* \right\Vert_1) \right) \right\} \implies \left\Vert \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}\right\Vert_1 \leq (1+\epsilon)\left\Vert \boldsymbol{\beta}_{\ell_1}^* \right\Vert_1
\end{align*}
\begin{align*}
    \alpha \geq \sqrt{2(1+ \epsilon)(1 + 2/\epsilon)\left\Vert \boldsymbol{\beta}_{\ell_2}^* \right\Vert_2} \implies \left\Vert \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}\right\Vert_2^2 \leq (1+\epsilon)\left\Vert \boldsymbol{\beta}_{\ell_2}^* \right\Vert_2^2.
\end{align*}
\end{manualtheorem}
Theorem \ref{woodworththm2} illustrates a challenge with approximating the rich limit: while only polynomially large $\alpha$ suffices to approximate $\boldsymbol{\beta}_{\ell_2}^*$, one must have \textit{exponentially small} $\alpha$ to approximate $\boldsymbol{\beta}_{\ell_1}^*$. As a result, experiments close to the rich limit may require very small initializations and, as a result, may be computationally intractable. The work from \cite{arora2019implicit} suggests a remedy to this problem: increasing the depth (i.e. degree of homogeneity) of the model. 

\subsection{The Relation of Kernel and Rich Limits to Model Generalization}
Interestingly, the kernel and rich limits are related to the generalizability of the model. Woodworth et al. illustrate this phenomenon by generating a training set $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N \sim \mathcal{N}(0, I)$ and $y_n \sim \mathcal{N}(\langle \boldsymbol{\beta}^*, \boldsymbol{x}_n \rangle, 0.01)$, where $\boldsymbol{\beta}^*$ is an $r^*$ sparse vector whose nonzero entries are equal to $1/\sqrt{r^*}$. The authors note that for $N \leq d$, gradient flow generally reaches a zero training error solution, but not all solutions generalize the same. In particular, they show that in the rich limit, $N = \Omega(r^*\log d)$ training points suffices for $\boldsymbol{\beta}_{\ell_1}^*$ to \enquote{generalize well}; in the kernel limit, though, good generalization requires $N = \Omega(d)$. This tells us that, in general, \textit{good generalization of our model requires us to train close to the rich regime}. As previously noted, though, one cannot take $\alpha$ to be arbitrary small: as $w(0) \rightarrow \vec{0}$, we reach a saddle point of the objective function. Thus, \cite{woodworth2020kernel} suggests working at the edge of the rich limit, ensuring generalizability of the solution while also making sure that the optimization problem is feasible. One can see this in section 7 of \cite{woodworth2020kernel}, where the authors perform experiements with neural networks on the MNIST and CIFAR10 datasets.

\subsection{Reconsidering $\boldsymbol{w}_0$}
So far, we have only considered the problem of $\boldsymbol{w}_0 = \vec{1}$. The authors give a more general statement of Theorem \ref{woodworththm1} as follows:
\begin{manualtheorem}{1}\label{woodworththm1_1}
For any $0 < \alpha < \infty$ and $\boldsymbol{w}_0$ with no zero entries, if the gradient flow solution $\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty}$ for the squared parameterization model satisfies $X\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty} = \boldsymbol{y}$, then 
\begin{align*}
    \boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty} = \underset{\boldsymbol{\beta}}{argmin} \ Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) \ \text{s.t.} \ X\boldsymbol{\beta} = \boldsymbol{y}
\end{align*}
where $Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \sum_{i=1}^d \alpha^2 \boldsymbol{w}_{0,i}^2 q\left(\frac{\boldsymbol{\beta}_i}{\alpha^2\boldsymbol{w}_{0,i}^2}\right)$ and $q(z) = \int_0^z \text{arcsinh}\left(\frac{u}{2} \right) du = 2 - \sqrt{4+z^2} + z \text{arcsinh}\left(\frac{z}{2} \right).$
\end{manualtheorem}
The authors then note that for small $z$, $q(z) = \frac{z^2}{4} + \mathcal{O}(z^4)$ and so for $\alpha \rightarrow \infty$
\begin{align*}
    Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \sum_{i=1}^d \frac{\boldsymbol{\beta}_i^2}{4\alpha^2\boldsymbol{w}_{0, i}^2} + \mathcal{O}(\alpha^{-6}).
\end{align*}
That is, $Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta})$ is proportional to the $\ell_2$ norm weighted by $\text{diag}(1/\boldsymbol{w}_0^2)$. Conversely, for large $\left| z \right|$, $q(z) = \left|z \right| \log \left|z \right| + \mathcal{O}(1/ \left| z \right|)$, and so as $\alpha \rightarrow 0$, 
\begin{align*}
    \frac{1}{\log(1/\alpha^2)}Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \sum_{i=1}^d \left| \boldsymbol{\beta}_i \right| + \mathcal{O}(1/\log(1/\alpha^2)).
\end{align*}
This tells us that in the rich limit, $Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta})$ is proportional to $\left\Vert \boldsymbol{\beta} \right\Vert_1 $ regardless of the shape of the initialization $\boldsymbol{w}_0$. In summary, the specific initialization $\boldsymbol{w}_0$ \textit{does} affect the implicit bias in optimization under the kernel regime, but \textit{not} in the rich limit. For neural networks with i.i.d. initialized units, the particular value of $\boldsymbol{w}_0$ is analogous to the distribution used to initialize each unit. That is, changing the initialization distribution changes the tangent kernel at initialization and thus the kernel regime behavior; it does not, however, affect the rich limit.

\subsection{Implicit Bias and Weights}
The authors consider the possibility that the implicit bias is minimizing the $\ell_2$ norm from initialization:
\begin{align*}
    \boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^R := F \left( \underset{\boldsymbol{w}}{\text{argmin}} \left\Vert \boldsymbol{w} - \alpha \boldsymbol{w}_0 \right\Vert_2^2 \ \text{s.t.} \ L(\boldsymbol{w}) = 0 \right) = \underset{\boldsymbol{\beta}}{\text{argmin}} \ R_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) \ \text{s.t.} \ X \boldymbol{\beta} = y
\end{align*}
\begin{align*}
    \text{where} \quad R_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \underset{\boldsymbol{w}}{\min} \left\Vert \boldsymbol{w} - \alpha \boldsymbol{w}_0 \right\Vert_2^2 \ \text{s.t.} \ F(\boldsymbol{w}) = \boldsymbol{\beta}.
\end{align*}

They remark that for the least-squares model $f(\boldsymbol{w}, \boldsymbol{x}) = \langle \boldsymbol{w}, \boldsymbol{x} \rangle$, we indeed have $\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty} = \boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{R}$. That is, the implicit bias is captured by $R_{\alpha, \boldsymbol{w}_0}$. For the two-homogeneous model under consideration, however, this is not the case. Indeed for the case of $\boldsymbol{w}_0 = \vec{1}$, we have $Q_{\alpha, \vec{1}}(\boldsymbol{\beta}) = R_{\alpha, \vec{1}}(\boldsymbol{\beta})$ for $\alpha \rightarrow 0$ and $\alpha \rightarrow \infty$. From Figure 2 of \cite{woodworth2020kernel}, we observe that, in general, $Q_{\alpha, \vec{1}}(\boldsymbol{\beta}) \neq R_{\alpha, \vec{1}}(\boldsymbol{\beta})$, and they are not rescalings of each other. In particular, $R_{\alpha, \vec{1}}(\boldsymbol{\beta})$ approaches the rich limit $\left\Vert \boldsymbol{\beta} \right\Vert_1$ polynomially in $\alpha$, while we know from previously that this is not the case for $Q_{\alpha, \vec{1}}(\boldsymbol{\beta})$.

\subsection{Higher Order Models}
As the authors note, deeper models correspond to higher order homogeneity (ex. a depth-$D$ ReLU neural network is $D$-homogeneous). Accordingly, they generalize their model to a depth-D diagonal neural network
\begin{align*}
    F_D(\boldsymbol{w}) = \boldsymbol{\beta}_{\boldsymbol{w}, D} = \boldsymbol{w}_+^D - \boldsymbol{w}_-^D \quad \text{and} \quad f_D(\boldsymbol{w}, \boldsymbol{x})  = \langle \boldsymbol{w}_+^D - \boldsymbol{w}_-^D, \boldsymbol{x} \rangle.
\end{align*}
As Woodworth et al. remark, this is simply a linear model with an unconventional parameterization, or a depth-$D$ matrix factorization problem with commutative (diagonal) measurement matrices and diagonal factor matrices as studied by \cite{arora2019implicit}. This formulation leads us to the following theorem:
\begin{manualtheorem}{3}
For any $0 < \alpha < \infty$ and $D \geq 3$, if $X \boldsymbol{\beta}_{\alpha, D}^{\infty} = y$, then
\begin{align*}
    \boldsymbol{\beta}_{\alpha, D}^{\infty} = \text{argmin}_{\boldsymbol{\beta}} Q_{\alpha}^D(\boldsymbol{\beta}) \ \text{s.t.} \ X \boldsymbol{\beta} = \boldsymbol{y}
\end{align*}
where $Q_{\alpha}^D(\boldymbol{\beta}) = \alpha^D \sum_{i=1}^d q_D(\boldsymbol{\beta}_i/\alpha^D)$ and $q_D = \int h_D^{-1}$ is the antiderivative of the unique inverse of $h_D(z) = (1-z)^{-\frac{D}{D-2}} - (1+z)^{-\frac{D}{D-2}}$ on $[-1, 1]$. Furthermore, $\lim_{\alpha \rightarrow 0} \boldsymbol{\beta}_{\alpha, D}^{\infty} = \boldsymbol{\beta}_{\ell_1}^*$ and $\lim_{\alpha \rightarrow \infty} \boldsymbol{\beta}_{\alpha, D}^{\infty} = \boldsymbol{\beta}_{\ell_2}^*$.
\end{manualtheorem}
The minimum $\ell_1$ solution in the rich limit for depth-$D$ diagonal neural networks has already been observed by Arora et al. While the rich and kernel limits do not change as the depth of the diagonal network increases, the transition between the rich and kernel regimes \textit{does} change. In particular, \textit{for higher orders of homogeneity, the transition into the extreme regimes is sharper}. Even at $D=3$, the scale of $\alpha$ needed to approximate $\left\Vert \boldsymbol{\beta}_{\ell_1}^* \right\Vert_1$ is polynomial rather than exponential in the case of $D=2$ (see Figure 3(b) in \cite{woodworth2020kernel}). 

\section{On Lazy Training in Differential Programming, Chizat, Oyallon, and Bach 2018}

Chizat's paper is particularly important as it provides much of the theoretical grounding upon which \cite{woodworth2020kernel} is built. 

\subsection{Lazy Training}
Let us consider parameter space $\mathbb{R}^p$, Hilbert space $\mathcal{F}$, a smooth model $h: \mathbb{R}^p \rightarrow \mathcal{F}$, and a smooth loss $R: \mathcal{F} \rightarrow \mathbb{R}_+$. Using gradient-based methods (i.e. gradient flow), the paper seeks to minimize the objective function $F: \mathbb{R}^p \mapsto \mathbb{R}_+$ defined
\begin{align*}
    F(w) := R(h(w)).
\end{align*}
With the initialization $w_0$, the authors also define the \textit{linearized model} $\overline{h}(w) = h(w_0) + Dh(w_0)(w - w_0)$ around $w_0$. The corresponding objective function is 
\begin{align*}
    \overline{F}(w) := R(\overline{h}(w)).
\end{align*}
At the beginning of training, it is known that the optimization paths of $F$ and $\overline{F}$ are close to each other. The \textit{lazy training} regime (what \cite{woodworth2020kernel} refers to as the \enquote{kernel regime}) occurs when the gradient flow dynamics of $F$ and $\overline{F}$ are close for \textit{all} $t \geq 0$, not just near $t = 0$.

A natural question to arise from this discussion is when the lazy training regime occurs. The authors provide a general criterion for the lazy regime, which relates the relative change of the objective $\Delta(F)$ to the relative change of the differential $\Delta(Dh)$.

In particular, assume that the initial set of parameters $w_0$ is not a minimizer so that $F(w_0) > 0$ and is not a critical point of the objective $F$ so that $\nabla F(w_0) \neq 0$. Consider the gradient descent step $w_1 = w_0 - \eta \nabla F(w_0)$ with small stepsize $\eta > 0$. The relative change of the objective for the step is $\Delta(F) := \frac{\left| F(w_1) - F(w_0) \right|}{F(w_0)} \approx \eta \frac{\left\Vert \nabla F(w_0) \right\Vert^2}{F(w_0)}$, and the relative change in the differential of $h$ measured in the operator norm is $\Delta(Dh) := \frac{\left\Vert Dh(w_1) - Dh(w_0) \right\Vert}{\left\Vert Dh(w_0) \right\Vert} \leq \eta \frac{\left\Vert \nabla F(w_0) \right\Vert \cdot \left\Vert D^2h(w_0) \right\Vert}{\left\Vert Dh(w_0)\right\Vert }$. Lazy training occurs when the \textit{differential of $h$ does not change significantly whereas the loss does} i.e. $\Delta(F) \gg \Delta(Dh)$. Using the above estimates for $\Delta(F), \Delta(Dh)$, this is guaranteed whenever
\begin{align*}
    \frac{\left\Vert \nabla F(w_0) \right\Vert}{F(w_0)} \gg \frac{\left\Vert D^2h(w_0)\right\Vert}{\left\Vert Dh(w_0)\right\Vert}.
\end{align*}
In the particular case where $R(y) = \frac{1}{2} \left\Vert y - y^{\star} \right\Vert^2$ for some $y^{\star} \in \mathcal{F}$ (i.e. the loss under consideration is the square loss), then we have the simpler criterion
\begin{align*}
    \kappa_h(w_0) := \left\Vert h(w_0) - y^{\star} \right\Vert \frac{\left\Vert D^2 h(w_0)\right\Vert}{\left\Vert D h(w_0)\right\Vert^2} \ll 1.
\end{align*}
This comes from the approximation $\left\Vert \nabla F(w_0) \right\Vert = \left\Vert Dh(w_0)^T(h(w_0) - y^{\star}) \right\Vert \approx \left\Vert Dh(w_0) \right\Vert \cdot \left\Vert h(w_0) - y^{\star} \right\Vert$.

\subsection{Scaling the Output}
Of particular importance to this work is how scaling the output $h(w)$ affects lazy training. Chizat and colleagues note that for the square loss, scaling the model $h$ by factor of $\alpha > 0$ results in 
\begin{align*}
    \kappa_{\alpha h}(w_0) = \frac{1}{\alpha} \left\Vert \alpha h(w_0) - y^{\star} \right\Vert \frac{\left\Vert D^2 h(w_0) \right\Vert}{\left\Vert Dh(w_0) \right\Vert^2}.
\end{align*}
And so as long as $\left\Vert \alpha h(w_0) - y^{\star} \right\Vert$ is bounded, taking $\alpha \rightarrow \infty$ leads to the lazy training regime. The authors discuss numerous strategies by which to ensure $h(w_0) = 0$, and so $\left\Vert \alpha h(w_0) - y^{\star} \right\Vert$ is indeed bounded.

For $D$-homogeneous models, multiplying the initialization $w_0$ by a factor of $\lambda$ is equivalent to scaling the output by $\lambda^D$. That is, 
\begin{align*}
    \kappa_{h}(\lambda w_0) = \frac{1}{\lambda^D} \left\Vert \lambda^D h(w_0) - y^{\star} \right\Vert \frac{\left\Vert D^2 h(w_0) \right\Vert}{\left\Vert Dh(w_0) \right\Vert^2}.
\end{align*}
Importantly, neural networks with homogeneous activation functions and linear, \textit{but not affine}, operators are important examples of homogeneous models.

\subsection{Lazy Training and the Scaled Model}
In the remainder of their work, Chizat et al. prove that for $\alpha > 0$ large, the training dynamics for the scaled objective
\begin{align*}
    F_{\alpha}(w) := \frac{1}{\alpha^2} R(\alpha h(w))
\end{align*}
are close to those for the linearized model
\begin{align*}
    \overline{F}_{\alpha}(w) := \frac{1}{\alpha^2} R(\alpha \overline{h}(w)),
\end{align*}
where $\overline{h}(w) := h(w_0) + Dh(w_0)(w - w_0)$ and $w_0 \in \mathbb{R}^p$ is a fixed initialization. That is, we prove the prior assertion that for scaling parameter $\alpha$ large, we indeed achieve lazy training as defined at the beginning of the paper. 

The necessary assumptions on the model $h: \mathbb{R}^p \rightarrow \mathcal{F}$ and loss $R: \mathcal{F} \rightarrow \mathbb{R}_+$ are as follows: the parametric model $h$ is differentiable with a locally Lipschitz differential $Dh$. Moreover, $R$ is differentiable with a Lipschitz gradient.

For the primary results of the paper, the authors consider the gradient flow of the objective function $F_{\alpha}$. For initialization $w_0$, the gradient flow is the path $(w_{\alpha}(t))_{t \geq 0}$ in the space of parameters $\mathbb{R}^p$ that satisfies $w_{\alpha}(0) = w_0$ and solves the ordinary differential equation
\begin{align*}
    w_{\alpha}'(t) = - \nabla F_{\alpha}(w_{\alpha}(t)) = - \frac{1}{\alpha} Dh(w_{\alpha}(t))^T \nabla R(\alpha h(w_{\alpha}(t))),
\end{align*}
where $Dh^T$ is the adjoint of the differential $Dh$. The authors compare this dynamic to the gradient flow $(\overline{w}_{\alpha}(t))_{t \geq 0}$ of $\overline{F}_{\alpha}$ that satisfies $(\overline{w}_{\alpha}(0) = w_0$ and solves 
\begin{align*}
    \overline{w}_{\alpha}'(t) = - \nabla \overline{F}_{\alpha}(\overline{w}_{\alpha}(t)) = - \frac{1}{\alpha} Dh(w_0)^T \nabla R(\alpha \overline{h}(\overline{w}_{\alpha}(t))).
\end{align*}

The first major result confirms that when $h(w_0) = 0$, then large $\alpha$ leads to lazy training. It is important to point out that here, we do not assume anything about the convexity of the loss $R$.
\begin{manualtheorem}{2.2}\label{Chizatthm2.2}
Assume that $h(w_0) = 0$. Given a fixed time horizon $T > 0$, it holds that $\sup_{t \in [0, T]} \left\Vert w_{\alpha}(t) - w_0 \right\Vert = \mathcal{O}(1/\alpha)$,
\begin{align*}
    \sup_{t \in [0, T]} \left\Vert w_{\alpha}(t) - \overline{w}_{\alpha}(t) \right\Vert = \mathcal{O}(1/\alpha^2) \quad \text{and} \quad  \sup_{t \in [0, T]} \left\Vert \alpha h(w_{\alpha}(t)) - \alpha \overline{h}(\overline{w}_{\alpha}(t)) \right\Vert = \mathcal{O}(1/\alpha^2).
\end{align*}
\end{manualtheorem}
Each of these three statements is of key importance. The first tells us that as $\alpha$ increases, $w_{\alpha}(t)$ is closer to initialization $w_{\alpha}(0)$ [in the $\ell_2$ norm]. The second tells us that the gradient flow of $F_{\alpha}$ approaches the gradient flow of $\overline{F}_{\alpha}$ as $\alpha$ grows. The last tells us that the scaled model $\alpha h$ approaches the scaled linearized model $\alpha \overline{h}$ for $\alpha$ large. The constants in Theorem \ref{Chizatthm2.2} depend \textit{exponentially on the time horizon $T$}.

In the next portion of their paper, the author gives \textit{uniform} bounds in time and convergence results under the assumption that the loss $R$ is strongly convex. By this assumption, $\overline{F}_{\alpha}$ is strictly convex on the affine hyperspace $w_0 + \text{ker} \ Dh(w_0)^{\perp}$ which contains the linearized gradient flow $(\overline{w}_{\alpha}(t))_{t \geq 0}$. Therefore, $(\overline{w}_{\alpha}(t))_{t \geq 0}$ converges linearly to the unique global minimizer of $\overline{F}_{\alpha}$ (since we assumed strong convexity of $R$). The authors first treat the overparameterized case, when $Dh(w_0)$ is surjective. As they discuss, $\text{rank} \ Dh(w_0)$ gives the degrees of freedom of the model around initialization $w_0$. 

\begin{manualtheorem}{2.4}
Consider the $M$-smooth and $m$-strongly convex loss $R$ with minimizer $y^{\star}$ and condition number $\kappa := M/m$. Assume that $\sigma_{\text{min}}$, the smallest singular value of $Dh(w_0)^T$, is positive and that the initialization satisfies $\left\Vert h(w_0) \right\Vert \leq C_0:= \sigma_{\text{min}}^3/(32\kappa^{3/2} \left\Vert Dh(w_0) \right\Vert \text{Lip}(Dh))$, where $\text{Lip}(Dh)$ is the Lipschitz constant of $Dh$. If $\alpha > \left\Vert y^{\star} \right\Vert / C_0$, then for $t \geq 0$, it holds
\begin{align*}
    \left\Vert \alpha h(w_{\alpha}(t)) - y^{\star} \right\Vert \leq \sqrt{\kappa} \left\Vert \alpha h(w_0) - y^{\star} \right\Vert \exp( -m \sigma_{\text{min}}^2 t/4).
\end{align*}
If moreover $h(w_0) = 0$, it holds as $\alpha \rightarrow \infty$, $\sup_{t \geq 0} \left\Vert w_{\alpha}(t) - w_0 \right\Vert = \mathcal{O}(1/\alpha)$,
\begin{align*}
    \sup_{t \geq 0} \left\Vert \alpha h(w_{\alpha}(t)) - \alpha \overline{h}(\overline{w}_{\alpha}(t)) \right\Vert = \mathcal{O}(1/\alpha) \quad \text{and} \quad  \sup_{t \geq 0} \left\Vert w_{\alpha}(t) - \overline{w}_{\alpha}(t) \right\Vert = \mathcal{O}(\log \alpha/\alpha^2).
\end{align*}
\end{manualtheorem}
Notice that there is no dependence here on the time horizon $T$, as there was in Theorem \ref{Chizatthm2.2}. Instead, we get a bound that is uniform in $t \geq 0$.

Removing the overparameterized assumption, the authors prove linear convergence of the gradient flow for large values of $\alpha$:
\begin{manualtheorem}{2.5}
Assume that $\mathcal{F}$ is separable, R is strongly convex, $h(w_0) = 0$, and $\text{rank} \ Dh(w)$ is constant on a neighborhood of $w_0$. Then there exists ${\alpha_0} > 0$ such that for all $\alpha > \alpha_0$, the gradient flow converges at a geometric rate (asymptotically independent of $\alpha$) to a local minimum of $F_{\alpha}$.
\end{manualtheorem}
And so we have that, for sufficiently large $\alpha$, $\lim_{t \rightarrow \infty} w_{\alpha}(t)$ is a strict local minimizer. It is not in general, though, a global minimizer of $F_{\alpha}$ \textit{because the image of $Dh(w_0)$ need not contain the global minimizer of $R$}.

\subsection{Applications to Neural Networks}
Of importance to our present research, Chizat and colleagues discuss how these theoretical results may be applied to deep leaning problems. In these problems, we typically have a function $f$ that maps from $\mathbb{R}^p \times \mathbb{R}^d$ to $\mathbb{R}^k$, where $\mathbb{R}^d$ is the input space and $\mathbb{R}^k$ is the output space. To relate this $f$ to the problem under consideration, let $\mathcal{F}$ be a Hilbert space of functions from $\mathbb{R}^d$ to $\mathbb{R}^k$. For example, we can have $\mathcal{F} = L^2(\rho_x, \mathbb{R}^k)$ where $\rho_x$ is the distribution of input samples. So then $h: \mathbb{R}^p \rightarrow L^2(\rho_x, \mathbb{R}^k)$ maps a vector of parameters to a predictor. That is, $h: w \mapsto f(w, \cdot)$.

One particular application is the two-layer neural network
\begin{align*}
    f_m(w, x) = \alpha(m) \sum_{j=1}^m b_j \cdot \sigma(a_j \cdot x),
\end{align*}
where $m$ is the size of the hidden layer, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ is an activation function, and $(\theta_j)_{j=1}^m$, $\theta_j = (a_j, b_j) \in \mathbb{R}^{d+1}$ are the parameters. Here $\alpha(m)$ is the scaling factor as previously discussed. Note that in order for $h$ to be differentiable, we must have that $\sigma$ is smooth.

\section{Neural Tangent Kernel: Convergence and Generalization in Neural Networks, Jacot, Gabriel, and Hongler 2018}

The paper by Jacot and colleagues studies the training of artificial neural networks in the infinite width limit. They consider fully-connected ANNs with layers numbered from $0$ (input) to $L$ (output), each containing $n_0, \ldots, n_L$ neurons, and with a Lipschitz, twice-differentiable nonlinearity function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. The authors remark that the smoothness assumption on $\sigma$, while useful for proving the desired results, do not seem to be necessary. Popular activation functions like the ReLU are clearly not twice-differentiable, and so it is reassuring that the result holds true outside of the differentiability assumptions. The paper focuses on what the authors call the ANN realization function $F^{(L)}: \mathbb{R}^p \rightarrow \mathcal{F}$, which maps parameters $\theta$ to functions $f_{\theta}$ in the space $\mathcal{F}$. As is true in a general fully-connected neural network, the parameters consist of the connection matrices $W^{(\ell)} \in \mathbb{R}^{n_{\ell} \times n_{\ell +1}}$ and bias vectors $b^{(\ell)} \in \mathbb{R}^{n_{\ell + 1}}$ for $\ell = 0, \ldots, L-1$. 

\subsection{The Kernel Gradient}
The training of an ANN involves optimizing $f_{\theta}$ in the function space $\mathcal{F}$ with respect to a functional cost $C: \mathcal{F} \rightarrow \mathbb{R}$. And even for a convex functional cost $C$, the composite cost $C \circ F^{(L)}: \mathbb{R}^p \rightarrow \mathbb{R}$ is in general highly non-convex. 

The authors show that during training, the network function $f_{\theta}$ follows descent along the kernel gradient with respect to the Neural Tangent Kernel (NTK). That is, during training the network function $f_{\theta}$ evolves along the negative kernel gradient
\begin{align*}
    \partial_t f_{\theta(t)} = - \nabla_{\Theta^{(L)}} C|_{f_{\theta(t)}}
\end{align*}
with respect to the \textit{neural tangent kernel} (NTK)
\begin{align*}
    \Theta^{(L)}(\theta) = \sum_{p=1}^P \partial_{\theta_p} F^{(L)}(\theta) \otimes \partial_{\theta_p} F^{(L)}(\theta).
\end{align*}
The realization function $F^{(L)}$ is not linear (in $\theta \in \mathbb{R}^p$). As a consequence, $\partial_{\theta_p} F^{(L)}(\theta)$, and thus the neural tangent kernel, depend on the parameters $\theta$. The NTK is random at initialization (since the authors initialize the parameters as i.i.d. $\mathcal{N}(0,1)$) and varies during training. 

For a more rigorous treatment of the kernel gradient, see section 3 of \cite{woodworth2020kernel}.

\subsection{Key Results}
Below, we summarize the principal results of the paper without 

\section{Implicit Regularization in Matrix Factorization, Gunasekar et al. 2017}
\subsection{Matrix Factorization Problem}
Consider the matrix factorization problem
\begin{align*}
    \underset{X \succeq 0}{\min} \ F(X) = \left\Vert \mathcal{A}(X) - y \right\Vert_2^2,
\end{align*}
where $\mathcal{A}: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^m$ is a linear operator specified by $\mathcal{A}(X)_i = \langle A_i, X \rangle$, $A_i \in \mathbb{R}^{n \times n}$ and $y \in \mathbb{R}^m$. Here $\langle A_i, X \rangle = \sum_{k=1}^n \sum_{j=1}^n \overline{(A_i)}_{kj} X_{kj}$ is the Frobenius inner product. Moreover, the authors restrict their attention to symmetric, positive semidefinite $X$ and symmetric, linearly independent $A_i$.

Instead of working with $X \in \mathbb{R}^{n \times n}$, however, the authors factor $X = UU^T$, where $U \in \mathbb{R}^{n \times d}$:
\begin{align*}
    \underset{X \succeq 0}{\min} \ f(X) = \left\Vert \mathcal{A}(UU^T) - y \right\Vert_2^2.
\end{align*}
Notice that by setting $d < n$, we are enforcing that matrix $X$ must be low-rank; for $d = n$, we reproduce the original problem.

\subsection{Underdetermined Problems and Gradient Descent}
Gunasekar and colleagues are particularly interested in the case of $m \ll n^2$ (the number of measurement matrices $A_i$ is much smaller than the number of entries in $X$). Under this regime, the problem is undetermined and there are many global minima satisfying $\mathcal{A}(X) = y$. Indeed, for sufficiently large $d$, the authors achieve zero training error in the underdetermined problem. What they are surprised by is that for $d > m/n$ with initialization $U_0$ close to zero and small stepsize, they still achieve small relative error. For $d < m/n$, the low-rank structure of $U$ can gaurantee generalization and reconstruction (see references in paper). 

By observing the nuclear norms of the solutions for the simulations, the authors hypothesize that the gradient descent solution is also the minimum nuclear norm solution when $U$ is full dimensional, the stepsize is sufficiently small, and the initialization approaches zero. That is, they suggest that under suitable conditions, the gradient descent solution satisfies
\begin{align*}
    \underset{X \succeq 0}{\text{argmin}} \ \left\Vert X \right\Vert_* \quad \text{s.t.} \ \mathcal{A}(X) = y.
\end{align*}
\cite{arora2019implicit} subsequently shows that this is not the case.

\subsection{Key Findings and Results}
Instead of proving the lofty previous result, the authors settle on the following conjecture:
\begin{conjecture}
For any full rank $X_{\text{init}}$, if $\widehat{X} = \lim_{a \rightarrow 0} X_{\infty}(\alpha X_{\text{init}})$ exists and is a global optima for the matrix factorization problem with $\mathcal{A}(\widehat{X}) = y$, then $\widehat{X} \in \text{argmin}_{X \succeq 0} \left \Vert X \right \Vert_* \ \text{s.t.} \ \mathcal{A}(X) = y$. 
\end{conjecture}
Here, $\lim_{\alpha \rightarrow 0} X_{\infty}(X_{\text{init}})$ is the limit point of the gradient flow dynamics
\begin{align*}
    \dot{X}_t = \mathcal{A}^*(r_t)X_t - X_t\mathcal{A}^*(r_t)
\end{align*}
initialized at $X_0 = X_{\text{init}}$. These dynamics are equivalent to the gradient flow on the matrix $U \in \mathbb{R}^{n \times d}$ in the matrix factorization problem:
\begin{align*}
    \dot{U}_t = \mathcal{A}^*(\mathcal{A}(U_t U_t^T) - y)U_t.
\end{align*}

Biting off a smaller portion of this conjecture, the authors prove the result for the case of $A_1, \ldots, A_m$ commutative matrices:
\begin{manualtheorem}{1}\label{gunasekharthm1}
In the case where the matrices $\{A_i\}_{i=1}^m$ commute, if $\widehat{X} = \lim_{\alpha \rightarrow 0} X_{\infty}(\alpha X_{\text{init}})$ exists and is a global optima for the matrix factorization problem with $\mathcal{A}(\widehat{X}) = y$, then $\widehat{X} \in \text{argmin}_{X \succeq 0} \left \Vert X \right \Vert_* \ \text{s.t.} \ \mathcal{A}(X) = y$.
\end{manualtheorem}

One should notice that linear regression falls under this case: when the initial matrix $X_{\text{init}}$ and \enquote{measurements} $A_i$ are diagonal matrices, then we have an equivalent parameterization of the vector least-squares problem in terms of \textit{squares of the least-squares coefficients}. \footnote{Note that, in this case, $A_1, \ldots, A_m$ are the observations and the diagonal entries of $X$ in terms of the \textit{squares of} least-squares coefficients.} Notice that we cannot work with the least-squares coefficients themselves since we require that $X$ is positive semidefinite. Recall that the least-squares problem is 
\begin{align*}
  \underset{\beta \in \mathbb{R}^n}{\text{argmin}} \left\Vert y - W\beta \right\Vert_2^2,
\end{align*}
where $W \in \mathbb{R}^{m \times n}$ is the matrix whose rows are the observations, $\beta \in \mathbb{R}^n$ is the least-squares coefficient vector, and $y\in \mathbb{R}^m$ is the response vector.

As a result, we have the following Corollary to Theorem \ref{gunasekharthm1}:
\begin{manualcorollary}{2}\label{gunasekharcor2}
Let $x_{\infty}(x_{\text{init}})$ be the limit point of gradient flow on $\min_{u \in \mathbb{R}^n} \left\Vert A x(u) - y\right\Vert_2^2$ with initialization $x_{\text{init}}$, where $x(u)_i = u_i^2$, $A \in \mathbb{R}^{m \times n}$, and $y \in \mathbb{R}^n$. If $\lim_{\alpha \rightarrow 0}x_{\infty}(\alpha \vec{1})$ exists and $A\widehat{x}=y$, then
$\widehat{x} \in \text{argmin}_{x \in \mathbb{R}_+^m} \left\Vert x\right\Vert_1 \ \text{s.t.} \ Ax = y.$
\end{manualcorollary}
Clearly, this result is crucial to the least-squares problem (as a \enquote{diagonal} neural network) considered in \cite{woodworth2020kernel}.

\section{Implicit Regularization in Deep Matrix Factorization, Arora et al. 2019}
\subsection{Matrix Completion Problem}
Given a matrix $W^* \in \mathbb{R}^{m \times n}$ and a randomly chosen subset of observed entries, recover the unseen entries of $W^*$. View each entry of $W^*$ as a data point: observed entries are the training set, unobserved entries are the test set. This is an undetermined system with multiple solutions. Previous work has shown that if we assume that $W^*$ is low rank, certain technical assumptions are met, and sufficiently many entries are observed, then we can achieve approximate or exact recovery of $W^*$. \cite{gunasekar2018implicit} conjectured that under suitable conditions, the gradient descent solution for the matrix factorization problem is (always) the minimum nuclear norm solution.

\subsection{Matrix Completion Problems as Neural Networks}
One can use shallow neural networks to solve matrix completion problems. Consider parameterizing the solution $W$ as the product of two matrices $W = W_2 W_1$ and optimizing the non-convex objective for fitting the observed entries. This optimization problem can be viewed as a depth-2 linear neural network (that is, the activation functions are linear and there are no added bias terms).

This two-layer problem can be naturally extended to the \textbf{deep matrix factorization problem} considered in the paper: for $W \in \mathbb{R}^{d \times d'}$, $d_1, \ldots, d_{N-1} \in \mathbb{N}$, we parameterize
\begin{align*}
    W = W_N W_{N-1} \cdots W_1,
\end{align*}
where $W_j \in \mathbb{R}^{d_j \times d_{j-1}}$ with $d_n = d, d_0 = d'$. $N$ is the \textit{depth} of the matrix factorization, matrices $W_1, \ldots, W_N$ are the \textit{factors}.

\subsection{Key Findings and Results}
The present paper considers whether the implicit regularization induced by a deep matrix factorization is stronger than the shallow factorization. Recall that \cite{gunasekar2018implicit} only considers the shallow factorization $X = UU^T$. The authors then study the scenario when the number of observed entries $m$ is too small for exact recovery. They show that, in this regime, matrix factorizations outperform minimum nuclear norm solutions. Further, there is a tendency towards low rank solutions $W$, which intensifies with the depth $N$ of the network. 

The first original result of the paper extends Theorem 1 from \cite{gunasekar2018implicit} for depth-$N$ matrix completion problems:
\begin{manualtheorem}{2}
Suppose $N \geq 3$ and the matrices $A_1, \ldots, A_m$ commute. Then if $\bar{W}_{\text{deep}} := \lim_{\alpha \rightarrow 0} W_{\text{deep}, \infty}(\alpha)$, where $ W_{\text{deep}, \infty}(\alpha) = W_N(t) W_{N-1}(t) \cdots W_1(t)$ with $W_j(0) = \alpha I$, $\dot{W}_j(t) = -\frac{\partial \phi}{\partial W_j}(W_1(t), \ldots, W_N(t))$ $t \geq 0$, exists and is a global optimum for
\begin{align*}
    \min_{W \in \mathcal{S}_+^d} l(W) := \frac{1}{2} \sum_{i=1}^m (y_i - \langle A_i, W \rangle)^2,
\end{align*}
where $A_1 \ldots, A_m \in \mathbb{R}^{d \times d}$ are symmetric and linearly independent (and $\mathcal{S}_+^d$ is the set of symmetric, positive semidefinite matrices) with $l(\bar{W}_{\text{deep}}) = 0$, then it holds that $\bar{W}_{\text{deep}} \in \text{argmin}_{W \in \mathcal{S}_+^d, l(W) = 0} \ \left\Vert W \right\Vert_*.$ Here, $\left\Vert \cdot \right\Vert_*$ denotes the nuclear norm.
\end{manualtheorem}

The authors then consider whether the gradient descent solution to the matrix factorization problem always tends to the minimum nuclear norm solution. They show that when the number of observed entries is sufficiently large relative to the rank of the matrix, factorizations of all depth admit solutions that tend to the minimum nuclear norm. However, when fewer entries are observed, neither shallow nor deep factorizations minimize the nuclear norm. This contradicts the previously mentioned conjecture made by \cite{gunasekar2018implicit}.

Arora and colleagues then prove the following result:
\begin{manualtheorem}{3}\label{arorathm3}
The signed singular values of the product matrix W(t) evolve by 
\begin{align*}
    \dot{\sigma}_r(t) = -N (\sigma_r^2(t))^{1-1/N} \cdot \langle \nabla l(W(t)), u_r(t), v_r^T(t) \rangle, \qquad r = 1, \ldots, \min\{d, d'\}.
\end{align*}
If the matrix factorization is non-degenerate i.e. has depth $N \geq 2$, the singular values need not be signed (we can assume $\sigma_r(t) \geq 0$ for all $t$).
\end{manualtheorem}
Theorem \ref{arorathm3} establishes a tendency of gradient descent for matrix factorization problems to low rank solutions, which intensifies with the depth $N$ of the network. This is because the dynamics promote solutions that have a few large singular values and many small ones; the gap grows more extreme the deeper the matrix factorization (see Figure 3 on p. 9).

\bibliographystyle{siam}
\bibliography{References/biblio}

\end{document}