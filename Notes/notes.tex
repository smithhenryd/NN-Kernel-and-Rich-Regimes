\documentclass{article}

\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% Formatting and images 
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{soul}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{csquotes}

%% Packages for mathematical typesetting
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{pgf}
\usepackage{comment}
\usepackage{float}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{bbm}

\newtheorem{manualtheoreminner}{Theorem}
\newenvironment{manualtheorem}[1]{%
  \renewcommand\themanualtheoreminner{#1}%
  \manualtheoreminner
}{\endmanualtheoreminner}

\newtheorem{manuallemmainner}{Lemma}
\newenvironment{manuallemma}[1]{%
  \renewcommand\themanuallemmainner{#1}%
  \manuallemmainner
}{\endmanualtheoreminner}

\newtheorem{manualcorollaryinner}{Corollary}
\newenvironment{manualcorollary}[1]{%
  \renewcommand\themanualcorollaryinner{#1}%
  \manualcorollaryinner
}{\endmanualcorollaryinner}

\newtheorem*{conjecture}{Conjecture}

% Title content
\title{\textbf{Notes on Kernel and Rich Limits\\ in Neural Network Training}}
\author[]{Henry Smith}
\affil[]{\normalsize Yale University}
\date{\today}

\begin{document}

\maketitle
\section{Kernel and Rich Regimes in Overparametrized Models, Woodworth et al. 2020}

\subsection{Problem Setup}
The paper considers models $f: \mathbb{R}^p \times \mathcal{X} \rightarrow \mathbb{R}$ which map parameters $\boldsymbol{w} \in \mathbb{R}^p$ and observations $\boldsymbol{x} \in \mathcal{X}$ to predictions $f(\boldsymbol{w}, \boldsymbol{x})$. Let $F(\boldsymbol{w}) \in \{f: \mathcal{X} \rightarrow \mathbb{R} \}$ be the predictor implemented by the parameters $\boldsymbol{w}$. Of particular interest to the authors are models which are linear in the observations $\boldsymbol{x}$ (but not necessarily in the parameters $\boldsymbol{w}$). Since, in this case, $F(\boldsymbol{w})$ is in the dual space of $\mathcal{X}$, then we have that by Riesz Representation, $F(\boldsymbol{w})(\boldsymbol{x}) = \langle\boldsymbol{\beta}_{\boldsymbol{w}}, \boldsymbol{x} \rangle$ for some $\boldsymbol{\beta}_{\boldsymbol{w}}$. The paper focuses on $D$-homogeneous models in the parameter space $\boldsymbol{w}$ as discussed in \cite{chizat2018lazy}, which satisfy $F(c \cdot \boldsymbol{w}) = c^D F(\boldsymbol{w})$ for any $c \in \mathbb{R}_+$. For $D$-homogeneous networks, it is important to note that scaling the output by a factor of $c > 0$, which is the focus of \cite{chizat2018lazy}, is equivalent to scaling the input by a factor of $c^{1/D}$. 

Further, the paper considers the square loss function $L(\boldsymbol{w}) = \widetilde{L}(F(\boldsymbol{w})) = \sum_{i=1}^N(f(\boldsymbol{w}, \boldsymbol{x}_n) - y_n)^2$ for the model $F$ over the training set $(\boldsymbol{x}_1, y_1), \ldots, (\boldsymbol{x}_N, y_N).$ Woodworth and colleagues minimize the loss $L$ using the gradient flow dynamics, which can be thought of as gradient descent with the stepsize $\eta$ limiting to 0. More explicitly, the gradient flow dynamics are
\begin{align*}
    \dot{\boldsymbol{w}}(t) = - \nabla L(\boldsymbol{w}(t)).
\end{align*}

The principal result of \cite{chizat2018lazy} is that, under suitable conditions, the gradient flow dynamics on $\boldsymbol{w}$ approach those of a linearized model as the scale of initialization approaches infinity. To capture the scale of initialization, the authors consider the parameter $\alpha \in \mathbb{R}_+$. For fixed initialization scale $\alpha$, let $\boldsymbol{w}_{\alpha, \boldsymbol{w}_0}(t)$ be the gradient flow path with the initial condition $\boldsymbol{w}_{\alpha, \boldsymbol{w}_0}(0) = \alpha \boldsymbol{w}(0)$.

Woodworth and colleagues are particularly interested in the case of  $N \ll p$, where there are many global minimizers of $L(\boldsymbol{w})$ with $L(\boldsymbol{w}) = 0$. That is, the model is overparameterized/underdetermined and so we can fit the observations exactly. Of particular interest to the paper is which of the global minimizers $\boldsymbol{w}_{\alpha, \boldsymbol{w}_0}^{\infty} := \lim_{t \rightarrow \infty} \boldsymbol{w}_{\alpha, \boldsymbol{w}_0}(t)$ gradient flow converges to. 

\subsection{Kernel Regime}
Locally, the gradient descent depends only on the first-order approximation in $\boldsymbol{w}$:
\begin{align*}
    f(\boldsymbol{w}, \boldsymbol{x}) = f(\boldsymbol{w}(t), \boldsymbol{x}) + \langle \boldsymbol{w} - \boldsymbol{w}(t), \phi_{\boldsymbol{w}(t)}(\boldsymbol{x}) \rangle + \mathcal{O}( \left\Vert \boldsymbol{w} - \boldsymbol{w}(t) \right\Vert^2),
\end{align*}
where $\phi_{\boldsymbol{w}(t)}(\boldsymbol{x}) = \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t), \boldsymbol{x})$ is the \textit{feature map} corresponding to the \textit{tangent kernel} $K_{\boldsymbol{w}(t)}(\boldsymbol{x}, \boldsymbol{x}') = \langle \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t), \boldsymbol{x}), \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t), \boldsymbol{x}')\rangle$. Clearly this approximation is linear in $\boldsymbol{w}$ but need not be linear in $\boldsymbol{x}$ if the underlying model $F(\boldsymbol{w})$ is not linear in $\boldsymbol{x}$.

\cite{chizat2018lazy} provides an in-depth look at the \textit{kernel regime} in which the gradient flow dynamics $\dot{w}(t)$ \textit{depend only on a linear function in $\boldsymbol{w}$}. This means that $\phi_{\boldsymbol{w}(t)}(\boldsymbol{x})$, and thus the tangent kernel, is constant throughout training. Under certain conditions on the $D$-homogeneous model $F(\boldsymbol{w})$ and the loss function $L$, the kernel regime manifests as $\alpha \rightarrow \infty$ in the initialization $\boldsymbol{w}_{\alpha}(0) = \alpha \boldsymbol{w}(0)$. Since this is a theoretical limit, we use the kernel regime to refer to the case in which the gradient flow dynamics are \enquote{close to} the linearized dynamics and so the \textit{tangent kernel does not change significantly} throughout training (see Theorem 2.4 in \cite{chizat2018lazy}).

In particular, under the kernel regime, training $f(\boldsymbol{w}, \boldsymbol{x})$ is equivalent to training the affine model $f_K(\boldsymbol{w}, \boldsymbol{x}) = \alpha^Df(\boldsymbol{w}(0), \boldsymbol{x}) + \langle \phi_{\boldsymbol{w}(0)}(\boldsymbol{x}), \boldsymbol{w} - \boldsymbol{w}(0) \rangle$ using kernelized gradient descent/flow with the kernel $K_{\boldsymbol{w}_0}$ and a bias term $f(\boldsymbol{w}_0, \boldsymbol{x})$. Observe that $\phi_{\boldsymbol{w}(t)}(\boldsymbol{x}) = \nabla_{\boldsymbol{w}(t)} f_K(\boldsymbol{w}, \boldsymbol{x}) = \phi_{\boldsymbol{w}(0)}(\boldsymbol{x})$, and so the tangent kernel is indeed constant throughout training. Minimizing the loss of this affine model with gradient flow reaches the solution nearest to initialization where distance is measured with respect to the RKHS norm determined by $K_0$. That is, $F(\boldsymbol{w}_{\alpha}^{\infty}) = \text{argmin}_h \left\Vert h - F(\apha \boldsymbol{w}_0) \right\Vert_{K_0} \ \text{s.t.} \ h(X) = \boldsymbol{y}$. Here, our RKHS norm is $\left\Vert g \right\Vert_{K_0} = \inf \{ \left\Vert \ell \right\Vert_2: \ell \in \mathbb{R}^p, g(\boldsymbol{x}) = \langle \ell, \phi_{\boldsymbol{w}(0)}(\boldsymbol{x}) \rangle, \ \forall \boldsymbol{x} \in \mathcal{X}\}$. \cite{chizat2018lazy} advises choosing an unbiased initialization $\boldsymbol{w}_0$ such that $F(\boldsymbol{w}_0) = 0$. This means that the bias term $\alpha^Df(\boldsymbol{w}(0), \boldsymbol{x})$ depending on the initialization scale vanishes. 

\subsection{Example: Linear Regression}
The primary contribution of the paper is the explicit characterization of the implicit bias when training with gradient descent as a function of $\alpha$, the scale of initialization, in the following least-squares problem:
For $\mathcal{X} = \mathbb{R}^d$, define the model
\begin{align*}
    f(\boldsymbol{w}, \boldsymbol{x}) = \sum_{i=1}^d (\boldsymbol{w}_{+, i}^2 - \boldsymbol{w}_{-, i}^2)\boldsymbol{x}_i = \langle \boldsymbol{\beta}_{\boldsymbol{w}}, \boldsymbol{x} \rangle, \quad \boldsymbol{w} = \begin{bmatrix}
                        \boldsymbol{w}_+ \\
                        \boldsymbol{w}_-
                        \end{bmatrix} \in \mathbb{R}^{2d},
    \quad \boldsymbol{\beta}_{\boldsymbol{w}} = \boldsymbol{w}_+^2 - \boldsymbol{w}_-^2
\end{align*}
where $\boldsymbol{z}^2$ for $\boldsymbol{z} \in \mathbb{R}^d$ denotes element-wise squaring. This model represents a \enquote{diagonal} neural network where each input component $\boldsymbol{x}_i$ is connected to a positive input unit with weight $\boldsymbol{w}_{+, i}^2$ and a negative input unit with weight $\boldsymbol{w}_{-, i}^2$. This differs from the least-squares problem discussed in \cite{gunasekar2018implicit} where there is a single set of positive weights representing the squared least-squares coefficients (i.e. we have model $f(\boldsymbol{w}, \boldsymbol{x}) = \sum_{i=1}^d \boldsymbol{w}_i^2\boldsymbol{x}_i$). The authors prefer the former formulation of the least-squares problem for two reasons: (1) by choosing $\boldsymbol{w}_0$ such that $\boldsymbol{w}_+ = \boldsymbol{w}_-$, then the model vanishes at initialization $F(\alpha \boldsymbol{w}) = 0$ without this being a saddle point of the objective and (2) the image of $F(\boldsymbol{w})$ is all linear functionals i.e. $\text{img}( F(\boldsymbol{w})) = \mathcal{X}^*$.

The authors study the underdetermined case $N \ll d$ when there are many solution vectors $\boldsymbol{\beta} \in \mathbb{R}^d$ that satisfy $X 
\boldsymbol{\beta} = \boldsymbol{y}$. Let $\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty}$ be the solution reached by gradient flow when initialized at $\boldsymbol{w}_+(0) = \boldsymbol{w}_-(0) = \alpha \boldsymbol{w}_0$. Woodworth and colleagues first consider the case of $\boldsymbol{w}_0 = \vec{1}$ and then generalize their results.

For $\boldsymbol{w}_0 = \vec{1}$, it is easy to verify that the tangent kernel at initialization is $K_{\boldsymbol{w}(0)}(\boldsymbol{x}, \boldsymbol{x}') = 8 \alpha^2 \langle \boldsymbol{x}, \boldsymbol{x}' \rangle$, which is simply a scaling of the standard inner product kernel (just write out $\nabla_{\boldsymbol{w}} f(\boldsymbol{w}, \boldsymbol{x}) |_{\boldsymbol{w} = \boldsymbol{w}(0)}$).
That is, we have $\left\Vert \boldsymbol{\beta} \right\Vert_{K_0} \propto \left\Vert \boldsymbol{\beta} \right\Vert_2$. And so in the kernel regime, $\boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}$ is equal to the minimum $\ell^2$ solution $\boldsymbol{\beta}_{\ell^2}^* := \text{argmin}_{X \boldsymbol{\beta} = y} \left\Vert \boldsymbol{\beta} \right\Vert_2$. Conversely, as $\alpha \rightarrow 0$, we approach what Woodworth and colleagues classify as the \enquote{rich limit.} Under the rich limit, the tangent kernel changes significantly during optimization, and the model $f(\boldsymbol{w}, \boldsymbol{x})$ is not linear in $\boldsymbol{w}$. For the linear regression problem under consideration with $\boldsymbol{w}_0 = \vec{1}$, \cite{gunasekar2018implicit} proves in Corollary \ref{gunasekharcor2} that $\lim_{\alpha \rightarrow 0} \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty} = \boldsymbol{\beta}_{\ell^1}^* = \text{argmin}_{X \boldsymbol{\beta} = y} \left\Vert \boldsymbol{\beta}\right\Vert_1$.

And so we understand that for the current model of interest with initialization $\boldsymbol{w}_0 = \vec{1}$, the gradient flow solution in the kernel regime $\alpha \rightarrow \infty$ is the minimum $\ell^2$ solution and the solution in the rich regime $\alpha \rightarrow 0$ is the minimum $\ell^1$ solution. 

\subsection{Interpolation Between the Rich and Kernel Regimes}
What the authors seek to understand, however, is how the gradient flow solution interpolates between the rich and kernel limits. That is, what can we say about the implicit bias when training the linear regression model for $\alpha$ small, for example? This is the subject of the following theorem:
\begin{manualtheorem}{1}\label{woodworththm1}
For any $0 < \alpha < \infty$, if the gradient flow solution $\boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}$ for the squared parameterization model satisfies $X\boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty} = \boldsymbol{y}$, then 
\begin{align*}
    \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty} = \underset{\boldsymbol{\beta}}{argmin} \ Q_{\alpha}(\boldsymbol{\beta}) \ \text{s.t.} \ X\boldsymbol{\beta} = \boldsymbol{y}
\end{align*}
where $Q_{\alpha}(\boldsymbol{\beta}) = \alpha^2 \sum_{i=1}^d q\left(\frac{\boldsymbol{\beta}_i}{\alpha^2}\right)$ and $q(z) = \int_0^z \text{arcsinh}\left(\frac{u}{2} \right) du = 2 - \sqrt{4+z^2} + z \text{arcsinh}\left(\frac{z}{2} \right).$
\end{manualtheorem}
See \cite{woodworth2020kernel} for further analysis of how $Q_{\alpha}(\boldsymbol{\beta})$ reproduces the $\ell^2$ and $\ell^1$ minimization problems as $\alpha \rightarrow \infty$ and $\alpha \rightarrow 0$, respectively. In Theorem \ref{woodworththm2}, Woodworth and colleagues quantify the scale of $\alpha$ necessary to guarantee approximation of the minimum $\ell^1$ and $\ell^2$ solutions:
\begin{manualtheorem}{2}\label{woodworththm2}
For any $0 < \epsilon < d$, under the setting of Theorem \ref{woodworththm1} with $\boldsymbol{w}_0 = \vec{1}$, 
\begin{align*}
    \alpha \leq \min \left\{ (2(1 + \epsilon)\left\Vert \boldsymbol{\beta}_{\ell^1}^* \right\Vert_1)^{-\frac{2+ \epsilon}{2\epsilon}}, \exp \left(-d/(\epsilon \left\Vert \boldsymbol{\beta}_{\ell^1}^* \right\Vert_1) \right) \right\} \implies \left\Vert \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}\right\Vert_1 \leq (1+\epsilon)\left\Vert \boldsymbol{\beta}_{\ell^1}^* \right\Vert_1
\end{align*}
\begin{align*}
    \alpha \geq \sqrt{2(1+ \epsilon)(1 + 2/\epsilon)\left\Vert \boldsymbol{\beta}_{\ell^2}^* \right\Vert_2} \implies \left\Vert \boldsymbol{\beta}_{\alpha, \vec{1}}^{\infty}\right\Vert_2^2 \leq (1+\epsilon)\left\Vert \boldsymbol{\beta}_{\ell^2}^* \right\Vert_2^2.
\end{align*}
\end{manualtheorem}
Theorem \ref{woodworththm2} illustrates a challenge with approximating the rich limit: while only polynomially large $\alpha$ suffices to approximate $\boldsymbol{\beta}_{\ell^2}^*$, one must have \textit{exponentially small} $\alpha$ to approximate $\boldsymbol{\beta}_{\ell^1}^*$. As a result, experiments close to the rich limit may require very small initializations and, as a result, may be computationally intractable. The work from \cite{arora2019implicit} suggests a remedy to this problem: increasing the depth (i.e. degree of homogeneity) of the model. 

\subsection{The Relation of Kernel and Rich Limits to Model Generalization}
Interestingly, the kernel and rich limits are related to the generalizability of the model. Woodworth et al. illustrate this phenomenon by generating a training set $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N \sim \mathcal{N}(0, I)$ and $y_n \sim \mathcal{N}(\langle \boldsymbol{\beta}^*, \boldsymbol{x}_n \rangle, 0.01)$, where $\boldsymbol{\beta}^*$ is an $r^*$ sparse vector whose nonzero entries are equal to $1/\sqrt{r^*}$. The authors note that for $N \leq d$, gradient flow generally reaches a zero training error solution, but not all solutions generalize the same. In particular, they show that in the rich limit, $N = \Omega(r^*\log d)$ training points suffices for $\boldsymbol{\beta}_{\ell^1}^*$ to \enquote{generalize well}; in the kernel limit, though, good generalization requires $N = \Omega(d)$. This tells us that, in general, \textit{good generalization of our model requires us to train close to the rich regime}. As previously noted, though, one cannot take $\alpha$ to be arbitrary small: as $w(0) \rightarrow \vec{0}$, we reach a saddle point of the objective function. Thus, \cite{woodworth2020kernel} suggests working at the edge of the rich limit, ensuring generalizability of the solution while also making sure that the optimization problem is feasible. One can see this in section 7 of \cite{woodworth2020kernel}, where the authors perform experiements with neural networks on the MNIST and CIFAR10 datasets.

\subsection{Reconsidering $\boldsymbol{w}_0$}
So far, we have only considered the problem of $\boldsymbol{w}_0 = \vec{1}$. The authors give a more general statement of Theorem \ref{woodworththm1} as follows:
\begin{manualtheorem}{1}\label{woodworththm1_1}
For any $0 < \alpha < \infty$ and $\boldsymbol{w}_0$ with no zero entries, if the gradient flow solution $\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty}$ for the squared parameterization model satisfies $X\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty} = \boldsymbol{y}$, then 
\begin{align*}
    \boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty} = \underset{\boldsymbol{\beta}}{argmin} \ Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) \ \text{s.t.} \ X\boldsymbol{\beta} = \boldsymbol{y}
\end{align*}
where $Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \sum_{i=1}^d \alpha^2 \boldsymbol{w}_{0,i}^2 q\left(\frac{\boldsymbol{\beta}_i}{\alpha^2\boldsymbol{w}_{0,i}^2}\right)$ and $q(z) = \int_0^z \text{arcsinh}\left(\frac{u}{2} \right) du = 2 - \sqrt{4+z^2} + z \text{arcsinh}\left(\frac{z}{2} \right).$
\end{manualtheorem}
The authors then note that for small $z$, $q(z) = \frac{z^2}{4} + \mathcal{O}(z^4)$ and so for $\alpha \rightarrow \infty$
\begin{align*}
    Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \sum_{i=1}^d \frac{\boldsymbol{\beta}_i^2}{4\alpha^2\boldsymbol{w}_{0, i}^2} + \mathcal{O}(\alpha^{-6}).
\end{align*}
That is, $Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta})$ is proportional to the $\ell^2$ norm weighted by $\text{diag}(1/\boldsymbol{w}_0^2)$. Conversely, for large $\left| z \right|$, $q(z) = \left|z \right| \log \left|z \right| + \mathcal{O}(1/ \left| z \right|)$, and so as $\alpha \rightarrow 0$, 
\begin{align*}
    \frac{1}{\log(1/\alpha^2)}Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \sum_{i=1}^d \left| \boldsymbol{\beta}_i \right| + \mathcal{O}(1/\log(1/\alpha^2)).
\end{align*}
This tells us that in the rich limit, $Q_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta})$ is proportional to $\left\Vert \boldsymbol{\beta} \right\Vert_1 $ regardless of the shape of the initialization $\boldsymbol{w}_0$. In summary, the specific initialization $\boldsymbol{w}_0$ \textit{does} affect the implicit bias in optimization under the kernel regime, but \textit{not} in the rich limit. For neural networks with i.i.d. initialized units, the particular value of $\boldsymbol{w}_0$ is analogous to the distribution used to initialize each unit. That is, changing the initialization distribution changes the tangent kernel at initialization and thus the kernel regime behavior; it does not, however, affect the rich limit.

\subsection{Implicit Bias and Weights}
The authors consider the possibility that the implicit bias is minimizing the $\ell^2$ norm from initialization:
\begin{align*}
    \boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^R := F \left( \underset{\boldsymbol{w}}{\text{argmin}} \left\Vert \boldsymbol{w} - \alpha \boldsymbol{w}_0 \right\Vert_2^2 \ \text{s.t.} \ L(\boldsymbol{w}) = 0 \right) = \underset{\boldsymbol{\beta}}{\text{argmin}} \ R_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) \ \text{s.t.} \ X \boldymbol{\beta} = y
\end{align*}
\begin{align*}
    \text{where} \quad R_{\alpha, \boldsymbol{w}_0}(\boldsymbol{\beta}) = \underset{\boldsymbol{w}}{\min} \left\Vert \boldsymbol{w} - \alpha \boldsymbol{w}_0 \right\Vert_2^2 \ \text{s.t.} \ F(\boldsymbol{w}) = \boldsymbol{\beta}.
\end{align*}

They remark that for the least-squares model $f(\boldsymbol{w}, \boldsymbol{x}) = \langle \boldsymbol{w}, \boldsymbol{x} \rangle$, we indeed have $\boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{\infty} = \boldsymbol{\beta}_{\alpha, \boldsymbol{w}_0}^{R}$. That is, the implicit bias is captured by $R_{\alpha, \boldsymbol{w}_0}$. For the two-homogeneous model under consideration, however, this is not the case. Indeed for the case of $\boldsymbol{w}_0 = \vec{1}$, we have $Q_{\alpha, \vec{1}}(\boldsymbol{\beta}) = R_{\alpha, \vec{1}}(\boldsymbol{\beta})$ for $\alpha \rightarrow 0$ and $\alpha \rightarrow \infty$. From Figure 2 of \cite{woodworth2020kernel}, we observe that, in general, $Q_{\alpha, \vec{1}}(\boldsymbol{\beta}) \neq R_{\alpha, \vec{1}}(\boldsymbol{\beta})$, and they are not rescalings of each other. In particular, $R_{\alpha, \vec{1}}(\boldsymbol{\beta})$ approaches the rich limit $\left\Vert \boldsymbol{\beta} \right\Vert_1$ polynomially in $\alpha$, while we know from previously that this is not the case for $Q_{\alpha, \vec{1}}(\boldsymbol{\beta})$.

\subsection{Higher Order Models}
As the authors note, deeper models correspond to higher order homogeneity (ex. a depth-$D$ ReLU neural network is $D$-homogeneous). Accordingly, they generalize their model to a depth-D diagonal neural network
\begin{align*}
    F_D(\boldsymbol{w}) = \boldsymbol{\beta}_{\boldsymbol{w}, D} = \boldsymbol{w}_+^D - \boldsymbol{w}_-^D \quad \text{and} \quad f_D(\boldsymbol{w}, \boldsymbol{x})  = \langle \boldsymbol{w}_+^D - \boldsymbol{w}_-^D, \boldsymbol{x} \rangle.
\end{align*}
As Woodworth et al. remark, this is simply a linear model with an unconventional parameterization, or a depth-$D$ matrix factorization problem with commutative (diagonal) measurement matrices and diagonal factor matrices as studied by \cite{arora2019implicit}. This formulation leads us to the following theorem:
\begin{manualtheorem}{3}
For any $0 < \alpha < \infty$ and $D \geq 3$, if $X \boldsymbol{\beta}_{\alpha, D}^{\infty} = y$, then
\begin{align*}
    \boldsymbol{\beta}_{\alpha, D}^{\infty} = \text{argmin}_{\boldsymbol{\beta}} Q_{\alpha}^D(\boldsymbol{\beta}) \ \text{s.t.} \ X \boldsymbol{\beta} = \boldsymbol{y}
\end{align*}
where $Q_{\alpha}^D(\boldymbol{\beta}) = \alpha^D \sum_{i=1}^d q_D(\boldsymbol{\beta}_i/\alpha^D)$ and $q_D = \int h_D^{-1}$ is the antiderivative of the unique inverse of $h_D(z) = (1-z)^{-\frac{D}{D-2}} - (1+z)^{-\frac{D}{D-2}}$ on $[-1, 1]$. Furthermore, $\lim_{\alpha \rightarrow 0} \boldsymbol{\beta}_{\alpha, D}^{\infty} = \boldsymbol{\beta}_{\ell^1}^*$ and $\lim_{\alpha \rightarrow \infty} \boldsymbol{\beta}_{\alpha, D}^{\infty} = \boldsymbol{\beta}_{\ell^2}^*$.
\end{manualtheorem}
The minimum $\ell^1$ solution in the rich limit for depth-$D$ diagonal neural networks has already been observed by Arora et al. While the rich and kernel limits do not change as the depth of the diagonal network increases, the transition between the rich and kernel regimes \textit{does} change. In particular, \textit{for higher orders of homogeneity, the transition into the extreme regimes is sharper}. Even at $D=3$, the scale of $\alpha$ needed to approximate $\left\Vert \boldsymbol{\beta}_{\ell^1}^* \right\Vert_1$ is polynomial rather than exponential in the case of $D=2$ (see Figure 3(b) in \cite{woodworth2020kernel}). 

\section{On Lazy Training in Differential Programming, Chizat, Oyallon, and Bach 2018}

Chizat's paper is particularly important as it provides much of the theoretical grounding upon which \cite{woodworth2020kernel} is built. 

\subsection{Lazy Training}
Let us consider parameter space $\mathbb{R}^p$, Hilbert space $\mathcal{F}$, a smooth model $h: \mathbb{R}^p \rightarrow \mathcal{F}$, and a smooth loss $R: \mathcal{F} \rightarrow \mathbb{R}_+$. Using gradient-based methods (i.e. gradient flow), the paper seeks to minimize the objective function $F: \mathbb{R}^p \mapsto \mathbb{R}_+$ defined
\begin{align*}
    F(w) := R(h(w)).
\end{align*}
With the initialization $w_0$, the authors also define the \textit{linearized model} $\overline{h}(w) = h(w_0) + Dh(w_0)(w - w_0)$ around $w_0$. The corresponding objective function is 
\begin{align*}
    \overline{F}(w) := R(\overline{h}(w)).
\end{align*}
At the beginning of training, it is known that the optimization paths of $F$ and $\overline{F}$ are close to each other. The \textit{lazy training} regime (what \cite{woodworth2020kernel} refers to as the \enquote{kernel regime}) occurs when the gradient flow dynamics of $F$ and $\overline{F}$ are close for \textit{all} $t \geq 0$, not just near $t = 0$.

A natural question to arise from this discussion is when the lazy training regime occurs. The authors provide a general criterion for the lazy regime, which relates the relative change of the objective $\Delta(F)$ to the relative change of the differential $\Delta(Dh)$.

In particular, assume that the initial set of parameters $w_0$ is not a minimizer so that $F(w_0) > 0$ and is not a critical point of the objective $F$ so that $\nabla F(w_0) \neq 0$. Consider the gradient descent step $w_1 = w_0 - \eta \nabla F(w_0)$ with small stepsize $\eta > 0$. The relative change of the objective for the step is $\Delta(F) := \frac{\left| F(w_1) - F(w_0) \right|}{F(w_0)} \approx \eta \frac{\left\Vert \nabla F(w_0) \right\Vert^2}{F(w_0)}$, and the relative change in the differential of $h$ measured in the operator norm is $\Delta(Dh) := \frac{\left\Vert Dh(w_1) - Dh(w_0) \right\Vert}{\left\Vert Dh(w_0) \right\Vert} \leq \eta \frac{\left\Vert \nabla F(w_0) \right\Vert \cdot \left\Vert D^2h(w_0) \right\Vert}{\left\Vert Dh(w_0)\right\Vert }$. Lazy training occurs when the \textit{differential of $h$ does not change significantly whereas the loss does} i.e. $\Delta(F) \gg \Delta(Dh)$. Using the above estimates for $\Delta(F), \Delta(Dh)$, this is guaranteed whenever
\begin{align*}
    \frac{\left\Vert \nabla F(w_0) \right\Vert}{F(w_0)} \gg \frac{\left\Vert D^2h(w_0)\right\Vert}{\left\Vert Dh(w_0)\right\Vert}.
\end{align*}
In the particular case where $R(y) = \frac{1}{2} \left\Vert y - y^{\star} \right\Vert^2$ for some $y^{\star} \in \mathcal{F}$ (i.e. the loss under consideration is the square loss), then we have the simpler criterion
\begin{align*}
    \kappa_h(w_0) := \left\Vert h(w_0) - y^{\star} \right\Vert \frac{\left\Vert D^2 h(w_0)\right\Vert}{\left\Vert D h(w_0)\right\Vert^2} \ll 1.
\end{align*}
This comes from the approximation $\left\Vert \nabla F(w_0) \right\Vert = \left\Vert Dh(w_0)^T(h(w_0) - y^{\star}) \right\Vert \approx \left\Vert Dh(w_0) \right\Vert \cdot \left\Vert h(w_0) - y^{\star} \right\Vert$.

\subsection{Scaling the Output}
Of particular importance to this work is how scaling the output $h(w)$ affects lazy training. Chizat and colleagues note that for the square loss, scaling the model $h$ by factor of $\alpha > 0$ results in 
\begin{align*}
    \kappa_{\alpha h}(w_0) = \frac{1}{\alpha} \left\Vert \alpha h(w_0) - y^{\star} \right\Vert \frac{\left\Vert D^2 h(w_0) \right\Vert}{\left\Vert Dh(w_0) \right\Vert^2}.
\end{align*}
And so as long as $\left\Vert \alpha h(w_0) - y^{\star} \right\Vert$ is bounded, taking $\alpha \rightarrow \infty$ leads to the lazy training regime. The authors discuss numerous strategies by which to ensure $h(w_0) = 0$, and so $\left\Vert \alpha h(w_0) - y^{\star} \right\Vert$ is indeed bounded.

For $D$-homogeneous models, multiplying the initialization $w_0$ by a factor of $\lambda$ is equivalent to scaling the output by $\lambda^D$. That is, 
\begin{align*}
    \kappa_{h}(\lambda w_0) = \frac{1}{\lambda^D} \left\Vert \lambda^D h(w_0) - y^{\star} \right\Vert \frac{\left\Vert D^2 h(w_0) \right\Vert}{\left\Vert Dh(w_0) \right\Vert^2}.
\end{align*}
Importantly, neural networks with homogeneous activation functions and linear, \textit{but not affine}, operators are important examples of homogeneous models.

\subsection{Lazy Training and the Scaled Model}
In the remainder of their work, Chizat et al. prove that for $\alpha > 0$ large, the training dynamics for the scaled objective
\begin{align*}
    F_{\alpha}(w) := \frac{1}{\alpha^2} R(\alpha h(w))
\end{align*}
are close to those for the linearized model
\begin{align*}
    \overline{F}_{\alpha}(w) := \frac{1}{\alpha^2} R(\alpha \overline{h}(w)),
\end{align*}
where $\overline{h}(w) := h(w_0) + Dh(w_0)(w - w_0)$ and $w_0 \in \mathbb{R}^p$ is a fixed initialization. That is, we prove the prior assertion that for scaling parameter $\alpha$ large, we indeed achieve lazy training as defined at the beginning of the paper. 

The necessary assumptions on the model $h: \mathbb{R}^p \rightarrow \mathcal{F}$ and loss $R: \mathcal{F} \rightarrow \mathbb{R}_+$ are as follows: the parametric model $h$ is differentiable with a locally Lipschitz differential $Dh$. Moreover, $R$ is differentiable with a Lipschitz gradient.

For the primary results of the paper, the authors consider the gradient flow of the objective function $F_{\alpha}$. For initialization $w_0$, the gradient flow is the path $(w_{\alpha}(t))_{t \geq 0}$ in the space of parameters $\mathbb{R}^p$ that satisfies $w_{\alpha}(0) = w_0$ and solves the ordinary differential equation
\begin{align*}
    w_{\alpha}'(t) = - \nabla F_{\alpha}(w_{\alpha}(t)) = - \frac{1}{\alpha} Dh(w_{\alpha}(t))^T \nabla R(\alpha h(w_{\alpha}(t))),
\end{align*}
where $Dh^T$ is the adjoint of the differential $Dh$. The authors compare this dynamic to the gradient flow $(\overline{w}_{\alpha}(t))_{t \geq 0}$ of $\overline{F}_{\alpha}$ that satisfies $\overline{w}_{\alpha}(0) = w_0$ and solves 
\begin{align*}
    \overline{w}_{\alpha}'(t) = - \nabla \overline{F}_{\alpha}(\overline{w}_{\alpha}(t)) = - \frac{1}{\alpha} Dh(w_0)^T \nabla R(\alpha \overline{h}(\overline{w}_{\alpha}(t))).
\end{align*}

The first major result confirms that when $h(w_0) = 0$, then large $\alpha$ leads to lazy training. It is important to point out that here, we do not assume anything about the convexity of the loss $R$.
\begin{manualtheorem}{2.2}\label{Chizatthm2.2}
Assume that $h(w_0) = 0$. Given a fixed time horizon $T > 0$, it holds that $\sup_{t \in [0, T]} \left\Vert w_{\alpha}(t) - w_0 \right\Vert = \mathcal{O}(1/\alpha)$,
\begin{align*}
    \sup_{t \in [0, T]} \left\Vert w_{\alpha}(t) - \overline{w}_{\alpha}(t) \right\Vert = \mathcal{O}(1/\alpha^2) \quad \text{and} \quad  \sup_{t \in [0, T]} \left\Vert \alpha h(w_{\alpha}(t)) - \alpha \overline{h}(\overline{w}_{\alpha}(t)) \right\Vert = \mathcal{O}(1/\alpha^2).
\end{align*}
\end{manualtheorem}
Each of these three statements is of key importance. The first tells us that as $\alpha$ increases, $w_{\alpha}(t)$ is closer to initialization $w_{\alpha}(0)$ [in the $\ell^2$ norm]. The second tells us that the gradient flow of $F_{\alpha}$ approaches the gradient flow of $\overline{F}_{\alpha}$ as $\alpha$ grows. The last tells us that the scaled model $\alpha h$ approaches the scaled linearized model $\alpha \overline{h}$ for $\alpha$ large. The constants in Theorem \ref{Chizatthm2.2} depend \textit{exponentially on the time horizon $T$}.

In the next portion of their paper, the author gives \textit{uniform} bounds in time and convergence results under the assumption that the loss $R$ is strongly convex. By this assumption, $\overline{F}_{\alpha}$ is strictly convex on the affine hyperspace $w_0 + \text{ker} \ Dh(w_0)^{\perp}$ which contains the linearized gradient flow $(\overline{w}_{\alpha}(t))_{t \geq 0}$. Therefore, $(\overline{w}_{\alpha}(t))_{t \geq 0}$ converges linearly to the unique global minimizer of $\overline{F}_{\alpha}$ (since we assumed strong convexity of $R$). The authors first treat the overparameterized case, when $Dh(w_0)$ is surjective. As they discuss, $\text{rank} \ Dh(w_0)$ gives the degrees of freedom of the model around initialization $w_0$. 

\begin{manualtheorem}{2.4}
Consider the $M$-smooth and $m$-strongly convex loss $R$ with minimizer $y^{\star}$ and condition number $\kappa := M/m$. Assume that $\sigma_{\text{min}}$, the smallest singular value of $Dh(w_0)^T$, is positive and that the initialization satisfies $\left\Vert h(w_0) \right\Vert \leq C_0:= \sigma_{\text{min}}^3/(32\kappa^{3/2} \left\Vert Dh(w_0) \right\Vert \text{Lip}(Dh))$, where $\text{Lip}(Dh)$ is the Lipschitz constant of $Dh$. If $\alpha > \left\Vert y^{\star} \right\Vert / C_0$, then for $t \geq 0$, it holds
\begin{align*}
    \left\Vert \alpha h(w_{\alpha}(t)) - y^{\star} \right\Vert \leq \sqrt{\kappa} \left\Vert \alpha h(w_0) - y^{\star} \right\Vert \exp( -m \sigma_{\text{min}}^2 t/4).
\end{align*}
If moreover $h(w_0) = 0$, it holds as $\alpha \rightarrow \infty$, $\sup_{t \geq 0} \left\Vert w_{\alpha}(t) - w_0 \right\Vert = \mathcal{O}(1/\alpha)$,
\begin{align*}
    \sup_{t \geq 0} \left\Vert \alpha h(w_{\alpha}(t)) - \alpha \overline{h}(\overline{w}_{\alpha}(t)) \right\Vert = \mathcal{O}(1/\alpha) \quad \text{and} \quad  \sup_{t \geq 0} \left\Vert w_{\alpha}(t) - \overline{w}_{\alpha}(t) \right\Vert = \mathcal{O}(\log \alpha/\alpha^2).
\end{align*}
\end{manualtheorem}
Notice that there is no dependence here on the time horizon $T$, as there was in Theorem \ref{Chizatthm2.2}. Instead, we get a bound that is uniform in $t \geq 0$. Also, we observe from the first statement that, for sufficiently large $\alpha$, the convergence of $(w_{\alpha}(t))_{t \geq 0}$ to the global minimum of $F_{\alpha}$, denoted $y^{\star}$, is exponential in $t$.

Removing the overparameterized assumption, the authors prove linear convergence of the gradient flow for large values of $\alpha$:
\begin{manualtheorem}{2.5}
Assume that $\mathcal{F}$ is separable, R is strongly convex, $h(w_0) = 0$, and $\text{rank} \ Dh(w)$ is constant on a neighborhood of $w_0$. Then there exists ${\alpha_0} > 0$ such that for all $\alpha > \alpha_0$, the gradient flow converges at a geometric rate (asymptotically independent of $\alpha$) to a local minimum of $F_{\alpha}$.
\end{manualtheorem}
And so we have that, for sufficiently large $\alpha$, $\lim_{t \rightarrow \infty} w_{\alpha}(t)$ is a strict local minimizer. It is not in general, though, a global minimizer of $F_{\alpha}$ \textit{because the image of $Dh(w_0)$ need not contain the global minimizer of $R$}.

\subsection{Applications to Neural Networks}
Of importance to our present research, Chizat and colleagues discuss how these theoretical results may be applied to deep leaning problems. In these problems, we typically have a function $f$ that maps from $\mathbb{R}^p \times \mathbb{R}^d$ to $\mathbb{R}^k$, where $\mathbb{R}^d$ is the input space and $\mathbb{R}^k$ is the output space. To relate this $f$ to the problem under consideration, let $\mathcal{F}$ be a Hilbert space of functions from $\mathbb{R}^d$ to $\mathbb{R}^k$. For example, we can have $\mathcal{F} = L^2(\rho_x, \mathbb{R}^k)$ where $\rho_x$ is the distribution of input samples. So then $h: \mathbb{R}^p \rightarrow L^2(\rho_x, \mathbb{R}^k)$ maps a vector of parameters to a predictor. That is, $h: w \mapsto f(w, \cdot)$.

One particular application is the two-layer neural network
\begin{align*}
    f_m(w, x) = \alpha(m) \sum_{j=1}^m b_j \cdot \sigma(a_j \cdot x),
\end{align*}
where $m$ is the size of the hidden layer, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ is an activation function, and $(\theta_j)_{j=1}^m$, $\theta_j = (a_j, b_j) \in \mathbb{R}^{d+1}$ are the parameters. Here $\alpha(m)$ is the scaling factor as previously discussed. Note that in order for $h$ to be differentiable, we must have that $\sigma$ is smooth.

\section{Neural Tangent Kernel: Convergence and Generalization in Neural Networks, Jacot, Gabriel, and Hongler 2018}

The paper by Jacot and colleagues studies the training of artificial neural networks in the infinite width limit. They consider fully-connected ANNs with layers numbered from $0$ (input) to $L$ (output), each containing $n_0, \ldots, n_L$ neurons, and with a Lipschitz, twice-differentiable nonlinearity function $\sigma: \mathbb{R} \rightarrow \mathbb{R}$. The authors remark that the smoothness assumption on $\sigma$, while useful for proving the desired results, do not seem to be necessary. Popular activation functions like the ReLU are clearly not twice-differentiable, and so it is reassuring that the result holds true outside of the differentiability assumptions. The paper focuses on what the authors call the ANN realization function $F^{(L)}: \mathbb{R}^p \rightarrow \mathcal{F}$, which maps parameters $\theta$ to functions $f_{\theta}$ in the space $\mathcal{F}$. As is true in a general fully-connected neural network, the parameters consist of the connection matrices $W^{(\ell)} \in \mathbb{R}^{n_{\ell} \times n_{\ell +1}}$ and bias vectors $b^{(\ell)} \in \mathbb{R}^{n_{\ell + 1}}$ for $\ell = 0, \ldots, L-1$. 

\subsection{The Kernel Gradient}
The training of an ANN involves optimizing $f_{\theta}$ in the function space $\mathcal{F}$ with respect to a functional cost $C: \mathcal{F} \rightarrow \mathbb{R}$. And even for a convex functional cost $C$, the composite cost $C \circ F^{(L)}: \mathbb{R}^p \rightarrow \mathbb{R}$ is in general highly non-convex. 

The authors show that during training, the network function $f_{\theta}$ follows descent along the kernel gradient with respect to the Neural Tangent Kernel (NTK). That is, during training the network function $f_{\theta}$ evolves along the negative kernel gradient
\begin{align*}
    \partial_t f_{\theta(t)} = - \nabla_{\Theta^{(L)}} C|_{f_{\theta(t)}}
\end{align*}
with respect to the \textit{neural tangent kernel} (NTK)
\begin{align*}
    \Theta^{(L)}(\theta) = \sum_{p=1}^P \partial_{\theta_p} F^{(L)}(\theta) \otimes \partial_{\theta_p} F^{(L)}(\theta).
\end{align*}
The realization function $F^{(L)}$ is not linear (in $\theta \in \mathbb{R}^p$). As a consequence, $\partial_{\theta_p} F^{(L)}(\theta)$, and thus the neural tangent kernel, depend on the parameters $\theta$. The NTK is random at initialization (since the authors initialize the parameters as i.i.d. $\mathcal{N}(0,1)$) and varies during training. 

In kernelized gradient descent with respect to the neural tangent kernel, the parameters are updated according to a training direction $d_t \in \mathcal{F}$:
\begin{align*}
    \partial_t \theta_p(t) =  \left \langle \partial_{\theta_p} F^{(L)}(\theta(t)), d_t \right\rangle_{p^{in}}
\end{align*}
where $\langle f, g \rangle_p^{in} = \mathbb{E}_{x \sim p^{in}} [f(x)^T g(x) ]$ and $p^{in}$ is a fixed distribution on the input space $\mathbb{R}^{n_0}$. For gradient descent, the training direction is taken to be $d_t = -d|_{f_{\theta(t)}}$. Here, $d|_{f_{\theta(t)}}$ is the dual element such that $\partial_f^{in} C|_{f_0} = \langle d|_{f_0}, \cdot \rangle_{p^{in}}$. The most important takeaway here is that \textit{we are performing gradient descent in a function space $\mathcal{F}$} rather than in Euclidean space.

For a more rigorous treatment of the kernel gradient, see section 3 of \cite{woodworth2020kernel}.

\subsection{Key Results}
Below, we summarize the principal results of the paper without focusing too much on the particular math. 

The first key result tells us that as the width of the ANN tends to infinity, the NTK converges in probability to an explicit deterministic limit.
\begin{manualtheorem}{1}
For a network of depth $L$ at initialization, with a Lipschitz nonlinearity $\sigma$, and in the limit as the layers width $n_1, \ldots, n_{L-1} \rightarrow \infty$, the NTK $\Tmega^{(L)}$ converges in probability to a deterministic limiting kernel
\begin{align*}
    \Theta^{(L)} \rightarrow \Theta_{\infty}^{(L)} \otimes Id_{n_L}.
\end{align*}
\end{manualtheorem}
A specific definition for this limiting kernel is given in the paper and is in terms of the Gaussian processes to which the output functions $f_{\theta, k}$ tend in the infinite width limit $n_1, \ldots, n_{L-1} \rightarrow \infty$.

The second main theorem discusses the behavior of the NTK during training. Particularly, in the infinite-width limit, the NTK remains asymptotically constant during training. Recall that this is exactly the \enquote{kernel regime} discussed in both \cite{woodworth2020kernel} and \cite{chizat2018lazy} where the model's gradient flow dynamic approaches that of a linearized model.
\begin{manualtheorem}{2}
Assume that $\sigma$ is a Lipschitz, twice differentiable nonlinearity function, with bounded second derivative. For any $T$ such that the integral $\int_0^T \left\Vert d_t \right\Vert_{p^{in}} \ dt$ stays stochastically bounded, as $n_1, \ldots, n_{L-1} \rightarrow \infty$ we have, uniformly for $t \in [0, T]$
\begin{align*}
    \Theta^{(L)}(t) \rightarrow \Theta_{\infty}^{(L)} \otimes Id_{n_L}.
\end{align*}
\end{manualtheorem}
As is discussed in section 3 of the paper, the convergence of the neural tangent kernel to a critical point of the cost function $C$ \textit{is guaranteed for positive definite kernels}. The authors note that the limiting NTK $\Theta_{\infty}$ is positive definite if the span of the derivatives $\partial_{\theta_p}F^{(L)}, \ p = 1, \ldots, P$ becomes dense in $\mathcal{F}$ with respect to the $p^{in}$ norm as the width grows to infinity.

\section{Implicit Regularization in Matrix Factorization, Gunasekar et al. 2017}
\subsection{Matrix Factorization Problem}
Consider the matrix factorization problem
\begin{align*}
    \underset{X \succeq 0}{\min} \ F(X) = \left\Vert \mathcal{A}(X) - y \right\Vert_2^2,
\end{align*}
where $\mathcal{A}: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^m$ is a linear operator specified by $\mathcal{A}(X)_i = \langle A_i, X \rangle$, $A_i \in \mathbb{R}^{n \times n}$ and $y \in \mathbb{R}^m$. Here $\langle A_i, X \rangle = \sum_{k=1}^n \sum_{j=1}^n \overline{(A_i)}_{kj} X_{kj}$ is the Frobenius inner product. Moreover, the authors restrict their attention to symmetric, positive semidefinite $X$ and symmetric, linearly independent $A_i$.

Instead of working with $X \in \mathbb{R}^{n \times n}$, however, the authors factor $X = UU^T$, where $U \in \mathbb{R}^{n \times d}$:
\begin{align*}
    \underset{X \succeq 0}{\min} \ f(X) = \left\Vert \mathcal{A}(UU^T) - y \right\Vert_2^2.
\end{align*}
Notice that by setting $d < n$, we are enforcing that matrix $X$ must be low-rank; for $d = n$, we reproduce the original problem.

\subsection{Underdetermined Problems and Gradient Descent}
Gunasekar and colleagues are particularly interested in the case of $m \ll n^2$ (the number of measurement matrices $A_i$ is much smaller than the number of entries in $X$). Under this regime, the problem is undetermined and there are many global minima satisfying $\mathcal{A}(X) = y$. Indeed, for sufficiently large $d$, the authors achieve zero training error in the underdetermined problem. What they are surprised by is that for $d > m/n$ with initialization $U_0$ close to zero and small stepsize, they still achieve small relative error. For $d < m/n$, the low-rank structure of $U$ can gaurantee generalization and reconstruction (see references in paper). 

By observing the nuclear norms of the solutions for the simulations, the authors hypothesize that the gradient descent solution is also the minimum nuclear norm solution when $U$ is full dimensional, the stepsize is sufficiently small, and the initialization approaches zero. That is, they suggest that under suitable conditions, the gradient descent solution satisfies
\begin{align*}
    \underset{X \succeq 0}{\text{argmin}} \ \left\Vert X \right\Vert_* \quad \text{s.t.} \ \mathcal{A}(X) = y.
\end{align*}
\cite{arora2019implicit} subsequently shows that this is not the case.

\subsection{Key Findings and Results}
Instead of proving the lofty previous result, the authors settle on the following conjecture:
\begin{conjecture}
For any full rank $X_{\text{init}}$, if $\widehat{X} = \lim_{a \rightarrow 0} X_{\infty}(\alpha X_{\text{init}})$ exists and is a global optima for the matrix factorization problem with $\mathcal{A}(\widehat{X}) = y$, then $\widehat{X} \in \text{argmin}_{X \succeq 0} \left \Vert X \right \Vert_* \ \text{s.t.} \ \mathcal{A}(X) = y$. 
\end{conjecture}
Here, $\lim_{\alpha \rightarrow 0} X_{\infty}(X_{\text{init}})$ is the limit point of the gradient flow dynamics
\begin{align*}
    \dot{X}_t = \mathcal{A}^*(r_t)X_t - X_t\mathcal{A}^*(r_t)
\end{align*}
initialized at $X_0 = X_{\text{init}}$. These dynamics are equivalent to the gradient flow on the matrix $U \in \mathbb{R}^{n \times d}$ in the matrix factorization problem:
\begin{align*}
    \dot{U}_t = \mathcal{A}^*(\mathcal{A}(U_t U_t^T) - y)U_t.
\end{align*}

Biting off a smaller portion of this conjecture, the authors prove the result for the case of $A_1, \ldots, A_m$ commutative matrices:
\begin{manualtheorem}{1}\label{gunasekharthm1}
In the case where the matrices $\{A_i\}_{i=1}^m$ commute, if $\widehat{X} = \lim_{\alpha \rightarrow 0} X_{\infty}(\alpha X_{\text{init}})$ exists and is a global optima for the matrix factorization problem with $\mathcal{A}(\widehat{X}) = y$, then $\widehat{X} \in \text{argmin}_{X \succeq 0} \left \Vert X \right \Vert_* \ \text{s.t.} \ \mathcal{A}(X) = y$.
\end{manualtheorem}

One should notice that linear regression falls under this case: when the initial matrix $X_{\text{init}}$ and \enquote{measurements} $A_i$ are diagonal matrices, then we have an equivalent parameterization of the vector least-squares problem in terms of \textit{squares of the least-squares coefficients}. \footnote{Note that, in this case, $A_1, \ldots, A_m$ are the observations and the diagonal entries of $X$ in terms of the \textit{squares of} least-squares coefficients.} Notice that we cannot work with the least-squares coefficients themselves since we require that $X$ is positive semidefinite. Recall that the least-squares problem is 
\begin{align*}
  \underset{\beta \in \mathbb{R}^n}{\text{argmin}} \left\Vert y - W\beta \right\Vert_2^2,
\end{align*}
where $W \in \mathbb{R}^{m \times n}$ is the matrix whose rows are the observations, $\beta \in \mathbb{R}^n$ is the least-squares coefficient vector, and $y\in \mathbb{R}^m$ is the response vector.

As a result, we have the following Corollary to Theorem \ref{gunasekharthm1}:
\begin{manualcorollary}{2}\label{gunasekharcor2}
Let $x_{\infty}(x_{\text{init}})$ be the limit point of gradient flow on $\min_{u \in \mathbb{R}^n} \left\Vert A x(u) - y\right\Vert_2^2$ with initialization $x_{\text{init}}$, where $x(u)_i = u_i^2$, $A \in \mathbb{R}^{m \times n}$, and $y \in \mathbb{R}^n$. If $\lim_{\alpha \rightarrow 0}x_{\infty}(\alpha \vec{1})$ exists and $A\widehat{x}=y$, then
$\widehat{x} \in \text{argmin}_{x \in \mathbb{R}_+^m} \left\Vert x\right\Vert_1 \ \text{s.t.} \ Ax = y.$
\end{manualcorollary}
Clearly, this result is crucial to the least-squares problem (as a \enquote{diagonal} neural network) considered in \cite{woodworth2020kernel}.

\section{Implicit Regularization in Deep Matrix Factorization, Arora et al. 2019}
\subsection{Matrix Completion Problem}
Given a matrix $W^* \in \mathbb{R}^{m \times n}$ and a randomly chosen subset of observed entries, recover the unseen entries of $W^*$. View each entry of $W^*$ as a data point: observed entries are the training set, unobserved entries are the test set. This is an undetermined system with multiple solutions. Previous work has shown that if we assume that $W^*$ is low rank, certain technical assumptions are met, and sufficiently many entries are observed, then we can achieve approximate or exact recovery of $W^*$. \cite{gunasekar2018implicit} conjectured that under suitable conditions, the gradient descent solution for the matrix factorization problem is (always) the minimum nuclear norm solution.

\subsection{Matrix Completion Problems as Neural Networks}
One can use shallow neural networks to solve matrix completion problems. Consider parameterizing the solution $W$ as the product of two matrices $W = W_2 W_1$ and optimizing the non-convex objective for fitting the observed entries. This optimization problem can be viewed as a depth-2 linear neural network (that is, the activation functions are linear and there are no added bias terms).

This two-layer problem can be naturally extended to the \textbf{deep matrix factorization problem} considered in the paper: for $W \in \mathbb{R}^{d \times d'}$, $d_1, \ldots, d_{N-1} \in \mathbb{N}$, we parameterize
\begin{align*}
    W = W_N W_{N-1} \cdots W_1,
\end{align*}
where $W_j \in \mathbb{R}^{d_j \times d_{j-1}}$ with $d_n = d, d_0 = d'$. $N$ is the \textit{depth} of the matrix factorization, matrices $W_1, \ldots, W_N$ are the \textit{factors}.

\subsection{Key Findings and Results}
The present paper considers whether the implicit regularization induced by a deep matrix factorization is stronger than the shallow factorization. Recall that \cite{gunasekar2018implicit} only considers the shallow factorization $X = UU^T$. The authors then study the scenario when the number of observed entries $m$ is too small for exact recovery. They show that, in this regime, matrix factorizations outperform minimum nuclear norm solutions. Further, there is a tendency towards low rank solutions $W$, which intensifies with the depth $N$ of the network. 

The first original result of the paper extends Theorem 1 from \cite{gunasekar2018implicit} for depth-$N$ matrix completion problems:
\begin{manualtheorem}{2}
Suppose $N \geq 3$ and the matrices $A_1, \ldots, A_m$ commute. Then if $\bar{W}_{\text{deep}} := \lim_{\alpha \rightarrow 0} W_{\text{deep}, \infty}(\alpha)$, where $ W_{\text{deep}, \infty}(\alpha) = W_N(t) W_{N-1}(t) \cdots W_1(t)$ with $W_j(0) = \alpha I$, $\dot{W}_j(t) = -\frac{\partial \phi}{\partial W_j}(W_1(t), \ldots, W_N(t))$ $t \geq 0$, exists and is a global optimum for
\begin{align*}
    \min_{W \in \mathcal{S}_+^d} l(W) := \frac{1}{2} \sum_{i=1}^m (y_i - \langle A_i, W \rangle)^2,
\end{align*}
where $A_1 \ldots, A_m \in \mathbb{R}^{d \times d}$ are symmetric and linearly independent (and $\mathcal{S}_+^d$ is the set of symmetric, positive semidefinite matrices) with $l(\bar{W}_{\text{deep}}) = 0$, then it holds that $\bar{W}_{\text{deep}} \in \text{argmin}_{W \in \mathcal{S}_+^d, l(W) = 0} \ \left\Vert W \right\Vert_*.$ Here, $\left\Vert \cdot \right\Vert_*$ denotes the nuclear norm.
\end{manualtheorem}

The authors then consider whether the gradient descent solution to the matrix factorization problem always tends to the minimum nuclear norm solution. They show that when the number of observed entries is sufficiently large relative to the rank of the matrix, factorizations of all depth admit solutions that tend to the minimum nuclear norm. However, when fewer entries are observed, neither shallow nor deep factorizations minimize the nuclear norm. This contradicts the previously mentioned conjecture made by \cite{gunasekar2018implicit}.

Arora and colleagues then prove the following result:
\begin{manualtheorem}{3}\label{arorathm3}
The signed singular values of the product matrix W(t) evolve by 
\begin{align*}
    \dot{\sigma}_r(t) = -N (\sigma_r^2(t))^{1-1/N} \cdot \langle \nabla l(W(t)), u_r(t)v_r^T(t) \rangle, \qquad r = 1, \ldots, \min\{d, d'\}.
\end{align*}
If the matrix factorization is non-degenerate i.e. has depth $N \geq 2$, the singular values need not be signed (we can assume $\sigma_r(t) \geq 0$ for all $t$).
\end{manualtheorem}
Theorem \ref{arorathm3} establishes a tendency of gradient descent for matrix factorization problems to low rank solutions, which intensifies with the depth $N$ of the network. This is because the dynamics promote solutions that have a few large singular values and many small ones; the gap grows more extreme the deeper the matrix factorization (see Figure 3 on p. 9).

\section{On the Global Convergence of Gradient Descent for Over-parameterized Models Using Optimal Transport, Chizat and Bach 2018}

\subsection{Problem Statement}
Chizat and Bach discuss the problem of finding an element in the Hilbert space $\mathcal{F}$ that minimizes a smooth, convex loss function $R: \mathcal{F} \rightarrow \mathbb{R}_+$ and that is a linear combination of a few elements from a large given parameterized set $\{ \phi(\theta) \}_{\theta \in \Theta} \subset \mathcal{F}$. They state that a general formulation of the problem is to describe the linear combination through an \textit{unknown signed measure $\mu$} on the parameter space and solve for
\begin{align*}
    J^* = \underset{\mu \in \mathcal{M}(\Theta)}{\min} \ J(\mu), \qquad J(\mu) := R \left( \int \phi d\mu \right) + G(\mu)
\end{align*}
where $\mathcal{M}(\Theta)$ is the set of signed measures on the parameter space $\Theta$ and $G: \mathcal{M}(\Theta) \rightarrow \mathbb{R}$ is an optional convex regularizer. In the present paper, the authors focus on the infinite-dimensional case where the parameter space is a domain of $\mathbb{R}^d$ and $\theta \rightarrow \phi(\theta)$ is differentiable.

While this framework may appear highly theoretical, it includes training neural works with a single hidden layer. Particularly, suppose we wish to select a function belonging to a certain class that maps features in $\mathbb{R}^{d-1}$ to labels in $\mathbb{R}$. In this case, we would have Hilbert space $\mathcal{F} = L^2(\mathbb{R}^{d-1}, \rho_x)$, quadratic or logistic loss function $R$, and $\phi(\theta): x \mapsto \sigma(\sum_{i=1}^{d-1} \theta_i x_i + \theta_d)$ with activation function $\theta: \mathbb{R} \rightarrow \mathbb{R}$

\subsection{Particle Gradient Descent}
The authors discuss particle gradient descent, a technique for finding approximate minimizers of the above problem. Particle gradient descent exploits the differentiability of $\phi$ and consists of discretizing the unknown measure $\mu$ as a mixture of $m$ particles parameterized by their positions and weights. This finite-dimensional problem is
\begin{align*}
    \underset{\boldsymbol{w} \in \mathbb{R}^m, \boldsymbol{\theta} \in \Theta^m}{\min} J_m(\boldsymbol{w}, \boldymbol{\theta}) \quad \text{where} \quad J_m(\boldsymbol{w}, \boldsymbol{\theta}) := J \left( \frac{1}{m} \sum_{i=1}^m w_i \delta_{\theta_i} \right)
\end{align*}
and can be solved with classical gradient-based algorithms.

The contributions of the paper are as follows: Chizat and Bach first introduce a more general class of problems and study the particle gradient flow for $m$ large (i.e. when there are many particles). They characterize the many particle limit as a \textit{Wasserstein gradient flow}. Then, under certain assumptions on $\phi$ and the initialization, they prove that if the Wasserstein gradient flow converges, then the limit is a global minimizer of $J$. Under these same assumptions, if
$(\boldsymbol{w}^{(m)}(t), \boldymbol{\theta}^{(m)}(t))_{t \geq 0}$ are gradient flows for $J_m$ suitably initialized, then 
\begin{align*}
    \lim_{m , t \rightarrow \infty} J(\mu_{m, t}) = J^* \quad \text{where} \quad \mu_{m,t} = \frac{1}{m} \sum_{i=1}^m w_i^{(m)}(t) \delta_{\theta_i^{(m)}(t)}.
\end{align*}
That is, in the infinite particle limit and as $t$ in the gradient flow tends to infinity, then the solution to the particle problem is also a solution (i.e. global minimizer) to the infinite-dimensional problem we originally stated.

\section{Universal Approximation Bounds for Superpositions of a Sigmoidal Function, Barron 1993}

\subsection{Main Results}
\begin{manualtheorem}{1}\label{barron1993thm1}
For every function in $\Gamma_{B, C}$, every sigmoidal function $\phi$, every probability measure $\mu$, and every $n \geq 1$, there exists a linear combination of sigmoidal functions $f_n(x)$ of the form
\begin{align*}
    f_n(x) = \sum_{k=1}^n c_k \phi(a_k \cdot x + b_k) + c_0
\end{align*}
such that
\begin{align*}
    \int_{B} (f(x) - f_n(x))^2 \mu(dx) \leq \frac{(2C)^2}{n}.
\end{align*}
The coefficients of the linear combination in $f_n$ may be restricted to satisfy $\sum_{k=1}^n \left|c_k \right| \leq 2C$ and $c_0 = f(0)$.
\end{manualtheorem}

\begin{manualtheorem}{4}
Let $f(x), x \in H$ be a function on a Hilbert space $H$ with $C_f = \int_H \left \omega \right| F(d\omega) < \infty$; then for every $r > 0$, every sigmoidal function $\phi$ on $\mathbb{R}$, every probability measure $\mu$ on $H$, and every $n \geq 1$, there is a linear combination of sigmoidal functions $f_n(x) = \sum_{k=1}^n c_k \phi(a_k \cdot x + b_k) + c_0$ such that 
\begin{align*}
\int_{B_r} (f(x) - f_n(x))^2 \mu(dx) \leq (2rC_f)^2/n,
\end{align*}
where $B_r = \{ x \in H: \left| x \right| \leq r \}$ is the Hilbert space ball of radius $r$.
\end{manualtheorem}

\section{Approximation and Estimation Bounds for Artificial Neural Networks, Barron 1993}

\subsection{Main Results}
The paper begins with a summary of the problem presented in \cite{barron1993universal}, but also makes an insightful remark: \enquote{In the case that $\hat{f}_n$ is a neural network function estimated from data, the norm $\Vert f - \hat{f}_n \Vert_{L_2(\mu, B)}$ measures the ability of the neural network to generalize to new data drawn with distribution $\mu$. In contrast, the empirical risk $(1/N) \sum_{i=1}^N (f(X_i) - f_n(X_i))^2$ only measures the accuracy of the observed data points $X_i, \ i = 1, 2, \ldots, N$.} 
Barron then remarks that the first step to obtain a bound on the statistical risk $\Vert f - \hat{f}_n \Vert$ is to bound the approximation error $\Vert f - f_n \Vert$ of the best neural network of size $n$. To do so, he uses a special case of Theorem \ref{barron1993thm1} from \cite{barron1993universal}:
\begin{manualtheorem}{1}
Given an arbitrary sigmoidal function $\phi$, an arbitrary target function $f$ with $C_f$ finite, and a probability measure $\mu$ on a domain $[-1, 1]^d$, then for every $n \geq 1$, there exists an artificial neural network of the form
\begin{align*}
    f_n(x) = f_n(x, \theta) = \sum_{k=1}^n c_k \phi(a_k \cdot x + b_k) + c_0
\end{align*}
such that 
\begin{align*}
    \Vert f - f_n \Vert \leq \frac{C_f}{\sqrt{n}}.
\end{align*}
For functions $f$ with $C_f \leq C$, the parameters may be restricted to satisfy $\sum_{k=1}^n \left| c_k \right| \leq C$, $\left|c_0 - f(0) \right| \leq C$, and $\left| b_k \right| \leq \left| a_k \right|_1$. 
\end{manualtheorem}

\begin{manualtheorem}{2}
Let a neural network be estimated by least squares with a complexity penalty (see pp. 121-122), where the range of $Y$ and each candidate function is restricted to a known interval of length $b$, then for any $\lambda > 5b^2/3$, for all $n \geq 1$, and all $N \geq 1$,
\begin{align*}
    \mathbb{E} \Vert f - \hat{f}_{n, N} \Vert \leq \gamma R_{n, N}(f) + \frac{2 \gamma \lambda}{N}
\end{align*}
and 
\begin{align*}
    \mathbb{E} \Vert f - \hat{f}_{N} \Vert \leq \gamma R_{N}(f) + \frac{2 \gamma \lambda}{N}
\end{align*}
where $\gamma = (3\lambda + b^2)/(3\lambda - 5b^2)$. Thus,
\begin{align*}
    \mathbb{E} \Vert f - \hat{f}_N \Vert \leq \mathcal{O}(R_N(f)).
\end{align*}
\end{manualtheorem}
Here, $\hat{f}_{n, N}$ is the least-squares estimator with a complexity penalty, and $\hat{f}_N = \hat{f}_{\hat{n}, N}$ is the minimum complexity estimator (we estimate both $n$ and $\theta$).

\section{Regularization Matters: Generalization and Optimization of
Neural Nets v.s. their Induced Kernel, Wei et al. 2020}

From \cite{jacot2018neural} we know that the prediction function found by gradient descent is in the span of the training data in a reproducing kernel Hilbert space induced by the neural tangent kernel. As the authors of the present paper note, though, the equivalence between gradient descent and the neural tangent kernel is broken once a regularization term is added. 

In understanding this distinction between kernelized gradient descent and gradient descent with a regularization term, the authors look at neural network generalization. Specifically, they construct a distribution of the data $\mathcal{D}$ such that a \enquote{two-layer neural network optimizing regularized logistic loss, will achieve a large margin, and therefore, good generalization.} Conversely, any prediction function in the span of the training data in the RKHS induced by the NTK will overfit the noise in the training data. This means that the solution to kernelized gradient descent will achieve poor margin and bad generalization.

\subsection{Generalization of Regularized Network versus NTK}

The distribution $\mathcal{D}$ of training data $(x,y)$ is as follows. For any $k \geq 3$, $x^Te_k \sim \{-1,1\}$ is a uniform random bit (i.e. the last $k-2$ components of $x$ are noise). And for $x^Te_1$, $x^Te_2$, $y$ choose
\begin{align*}
    &y = +1, \quad x^Te_1 = +1, \quad x^Te_2 = 0 \quad \text{w/ prob. 1/4}\\
    &y = +1, \quad x^Te_1 = +1, \quad x^Te_2 = 0 \quad \text{w/ prob. 1/4}\\
    &y = -1, \quad x^Te_1 = 0, \quad x^Te_2 = +1 \quad \text{w/ prob. 1/4}\\
    &y = -1, \quad x^Te_1 = 0, \quad x^Te_2 = -1 \quad \text{w/ prob. 1/4}
\end{align*}

The network computes $f^{\text{NN}}(x; \Theta) := \sum_{j=1}^m w_j [u_j^Tx]$, where $\Theta$ is the parameter space with $\theta_j = (u_j, w_j)$ and  $[\cdot]_+$ is the ReLU activation. The $\ell^2$ regularized logistic loss is 
\begin{align*}
    L_{\lambda}(\boldsymbol{\Theta}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-y_i f^{\text{NN}}(x_i, \Theta))) + \lambda \Vert \Theta \Vert_{\text{F}}^2.
\end{align*}
Note that the Frobenius norm $\Vert \Theta \Vert_{\text{F}}^2$ can be interpreted as the $\ell^2$ norm when the parameters are arranged into a single vector.

Let $\Theta_{\lambda} \in \text{argmin}_{\Theta} L_{\lambda}(\Theta)$ be the global optimizer. Also we know that the neural tangent kernel associated with the architecture is
\begin{align*}
   K(x, x') = \mathbb{E}_{w \sim \mathcal{N}(0, r_w^2), u \sim \mathcal{N}(0, r_u^2I)}[\langle \nabla_{\theta}f^{\text{NN}}(x; \Theta), \nabla_{\theta}f^{\text{NN}}(x'; \Theta)  \rangle].
\end{align*}
Notice that the expectation is taken over the weights of the network since they are initialized randomly. This is \textit{not} the case with \cite{woodworth2020kernel}, where the initial weights are chosen to be $\boldsymbol{w}_0 = \alpha \vec{1}$. That is, the NTK is deterministic--not random--upon initialization. For coefficients $\beta$, the prediction function $f^{\text{kernel}}(x; \beta)$ in the RKHS induced by $K$ can be written as $f^{\text{kernel}}(x; \beta) = \sum_{i=1}^n \beta_i K(x_i, x)$. 

This formulation of the problem leads us to present the main of the paper:
\begin{manualtheorem}{2.1}
Let \mathcal{D} be the distribution defined previously. With probability $1 - d^{-5}$ over the random draw of $n \lesssim d^2$ samples $(x_1, y_1), \ldots, (x_n, y_n)$ from $\mathcal{D}$, for all choices of $\beta$, the kernel prediction function $f^{\text{kernel}}(\cdot, \beta)$ will have at least $\Omega(1)$ error:
\begin{align*}
    \underset{(x, y) \sim \mathcal{D}}{\mathbb{P}}[f^{\text{kernel}}(x; \beta)y \leq 0] = \Omega(1).
\end{align*}
Meanwhile, for $\lambda \leq \text{poly}(n)^{-1}$, the regularized neural net solution $f^{\text{NN}}(\cdot; \Omega_{\lambda})$ with at least four hidden units can have good approximation with $\mathcal{O}(d^2)$ samples because we have the following generalization error bound:
\begin{align*}
    \underset{(x,y) \sim \mathcal{D}}{\mathbb{P}}[f^{\text{NN}}(x;\Theta_{\lambda})y \leq 0] \lesssim \sqrt{\frac{d}{n}}.
\end{align*}
This implies a $\Omega(d)$ sample-complexity gap between the regularized neural net and kernel prediction function.
\end{manualtheorem}
Notice here that $f^{\text{NN}}(x;\Theta_{\lambda})y > 0$ indicates good prediction of our network, whereas $f^{\text{NN}}(x;\Theta_{\lambda})y < 0$ indicates poor prediction. The authors' intuition is for this generalization gap is that the regularization term $\lambda \Vert \Theta \Vert_{\text{F}}^2$ \enquote{allows the neural net to find informative features
(weight vectors), that are adaptive to the data distribution and easier for the last layer’s weights to separate.} In particular, the weight vectors $[ e_1x ]_+$, $[-e_1x ]_+$, $[e_2x ]_+$, and $[-e_2x ]_+$, which determine the sign of the first two coordinates of $x$, are sufficient to fit the data distribution $\mathcal{D}$. Conversely, the kernel regime has a fixed feature space (namely, the feature map defined by $\varphi(x) = \nabla_{\theta}f^{\text{NN}}(x; \Theta)$) and is only searching for the coefficients in the RKHS.

\subsection{Perturbed Wasserstein Gradient Flow}
The principal result of the paper requires attaining the global minimum of the $\ell^2$ regularized loss. The authors show that for infinite-width two-layer networks, gradient descent finds global optimizers of the $\ell^2$ regularized loss in polynomial iterations. \cite{chizat2018global} demonstrated that as the hidden layer size approaches infinity, then gradient descent for a finite neural network approaches the \textit{Wasserstein gradient flow} over distributions of hidden units. 

The modeling of an infinite-width neural network is as follows. Define the following functional over distributions $\rho$ on $\mathbb{R}^{d+1}$: $L[\rho] := R(\int \Phi d\rho) + \int V d\rho$. Here, $\Phi: \mathbb{R}^{d+1} \rightarrow \mathbb{R}^k$ can be thought of the network function mapping a set of parameters to the predictions. Note that this differs from our common conception of neural networks as mapping from an input space to an output space. Similarly, $R: \mathbb{R}^k \rightarrow \mathbb{R}$ and $V: \mathbb{R}^{d+1} \rightarrow \mathbb{R}$ can be thought of the network loss and the regularizer, respectively. 

For the purposes of their problem, the authors take $\Phi$ and $V$ to be two-homogeneous. They require that $R$ is convex and nonnegative on the unit sphere and that $V$ is positive on the unit sphere. As for regularity assumptions, they specify that $\Phi$ and $V$ are differentiable as well as upper bounded and Lipschitz on the unit sphere. Moreover, they suppose that $R$ is Lipschitz and that its Hessian has bounded operator norm.

Now that we have stated this mathematical definition for neural networks, we relate the definition to the logistic regression problem under consideration. As we have previously suggested, $\rho$ has the interpretation of being a distribution over the parameters of the neural network. Take $k = n$ and $\Phi_i(\theta) = w \phi(u^T x_i)$ for $\theta = (w, u)$. Then $\int \Phi d\rho$ is a distributional neural network that computes an output for each of the $n$ training samples. The distributional version of the regularized logistic loss can be formulated by setting $V(\theta) := \lambda \Vert \theta \Vert_2^2$ and $R(a_1, \ldots, a_n) := \sum_{i=1}^n \log(1 + \exp(-y_ia_i))$.

From here, the authors define $L'[\rho]: \mathbb{R}^{d+1} \rightarrow \mathbb{R}$ such that $L'[\rho](\theta) := \langle R'(\int \Phi d\rho), \Phi(\theta) \rangle + V(\theta)$ and $v[\rho](\theta) := - \nabla_{\theta} L'[\rho](\theta)$. They mention that, informally, $L'[\rho]$ is the gradient of $L$ with respect to $\rho$, and $v$ is the induced velocity field. For the standard Wasserstein gradient flow dynamics, we have that $\rho_t$ evolves according to 
\begin{align*}
\frac{d}{dt}\rho_t = -\nabla \cdot (v[\rho_t] \rho_t).
\end{align*}
Notice that this gradient flow is over distributions on the parameter space (which is different than how we are accustomed to thinking about gradient flow). The authors then propose the modified dynamics 
\begin{align*}
    \frac{d}{dt}\rho_t = - \sigma \rho_t  + \sigma U^d  - \nabla \cdot (v[\rho_t] \rho_t)
\end{align*}
where $U^d$ is the uniform distribution over $\mathbb{S}^d$. The authors justify this small uniform noise by stating that it \enquote{ensures that at all time-steps, there is sufficient mass in a descent direction for the algorithm to decrease the objective.}

They then prove the following result, which gives convergence to a global optimizer of the regularized loss in polynomial time:
\begin{manualtheorem}{3.3}
Suppose that $\Phi$ and $V$ are two homogeneous and the aforementioned regularity conditions ($\Phi$ and $V$ differentiable, upper bounded and Lipschitz on $\mathbb{S}^d$, $R$ is Lispchitz and has Hessian with bounded operator norm). Also assume that for a starting distribution $\rho_0$, a solution to the perturbed Wasserstein gradient flow dynamics exist. Define $L^{\star} = \inf{\rho} L[\rho]$. Let $\epsilon > 0$ be a desired error threshold and choose $\sigma = \exp(-d\log(1/\epsilon)\text{poly}(k, L[\rho_0] - L^{\star}))$ and $t_{\epsilon} := \frac{d^2}{\epsilon^4} \text{poly}(\log(1/\epsilon), k, L[\rho_0] - L^{\star})$, where the regularity parameters for $\Phi$, $V$, and $R$ are hidden in $\text{poly}(\cdot)$. Then the perturbed Wasserstein gradient flow converges to an $\epsilon$-approximate global minimum in $t_{\epsilon}$ time:
\begin{align*}
    \underset{0 \leq t \leq t_{\epsilon}}{L[\rho_t]} - L^{\star} \leq \epsilon.
\end{align*}

\end{manualtheorem}


\section{A Priori Estimates of the Population Risk for Two-layer Neural Networks, Weinan et al. 2018}

The authors consider the problem of learning a function from a training set of $n$ \textit{i.i.d.} samples $\{(x_i, y_i)\}_{i=1}^n$ drawn from an underlying distribution $\rho_{x,y}$. The target function is $f^*(x) = \mathbb{E}[y|x]$. Further, the authors assume that the values $y_i$ are given by the decomposition $y = f^*(x) + \xi$, where $\xi$ denotes the noise. For simplicity of their analysis, the authors assume that the data lie in $X = [-1, 1]^d$ and $0 \leq f^* \leq 1$.

The network under consideration is 
\begin{align*}
    f(x; \theta) = \sum_{k=1}^m a_k \sigma(w_k^Tx),
\end{align*}
where $w_k \in \mathbb{R}^d$, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$ is a nonlinear, scale-invariant activation function (ex. ReLU). The authors consider $f$ to be overparameterized in the case of $m > n$ (that is, when the width of the network is greater than the dimension of the input).

In order to estimate the population risk
\begin{align*}
    L(\theta) = \mathbb{E}_{x,y}[\ell(f(x; \theta), y)],
\end{align*}
the authors work with the empirical risk
\begin{align*}
    \hat{L}_n(\theta) = \frac{1}{n} \sum_{i=1}^n \ell(f(x_i; \theta), y_i).
\end{align*}
where $\ell(y, y') = (y - y')^2$ is the squared loss. Moreover, they consider the path norm
\begin{align*}
    \left\Vert \theta \right\Vert_{\mathcal{P}} = \sum_{k=1}^m |a_k| \left\Vert w_k \right\Vert_1.
\end{align*}
For the two-layer network $f(\cdot, \theta)$ with width $m$, they then define the regularized risk to be 
\begin{align*}
    J_{\lambda}(\theta) := \hat{L}_n(\theta) + \lambda(\left\Vert \theta \right\Vert_{\mathcal{P}} + 1).
\end{align*}
They remark that the additional $\lambda$ terms is used to simplify their proof and does not affect their results. Corresponding to this definition, we can then define the regularized risk estimator to be 
\begin{align*}
    \hat{\theta}_{n, \lambda} = \text{argmin} J_{\lambda}(\theta).
\end{align*}
Note that $\hat{\theta}_{n, \lambda}$ is not necessarily unique.

\subsection{Barron Spaces}

Let $\mathbb{S}^d = \{ w| \left\Vert w \right\Vert_1 = 1\}$, and let $\mathcal{F}$ be the Borel $\sigma$-algebra on $\mathbb{S}^d$. Further, let $P(\mathbb{S}^d)$ denote the collection of all probability measures on $(\mathbb{S}^d, \mathcal{F})$. 

Now, we define $\mathcal{B}(X)$ to be the set of functions that admit the integral representation 
\begin{align*}
    f(x) = \int_{\mathbb{S}^d} a(w) \sigma(\langle w, x \rangle) d \pi(w) \qquad \forall x \in X,
\end{align*}
where $\pi \in P(\mathbb{S}^d)$, and $a(\cdot)$ is a measurable function with respect to $(\mathbb{S}^d, \mathcal{F})$. We can define a norm on $\mathcal{B}(X)$ for each $p \geq 1$ as follows: for any $f \in \mathcal{B}(X)$, let 
\begin{align*}
    \gamma_p(f) := \underset{(a, \pi) \in \Omega_f}{\inf} \left( \int_{\mathbb{S}^d} |a(w)|^p dw \right)^{1/p},
\end{align*}
where
\begin{align*}
    \Omega_f = \bigg\{(a, \pi) |f(x) = \int_{\mathbb{S}^d} a(w) \sigma(\langle w, x \rangle) d \pi(w) \bigg\}.
\end{align*}

Moreover, using this norm, we define the Barron space to be $B_p(X) := \{ f \in \mathcal{B}(X) | \gamma_p(f) < \infty\}$. Note that by Hölder's inequality, since $\pi(\cdot)$ is a probability distribution, then for any $q \geq p > 0$, we get $\gamma_p(f) \leq \gamma_q(f)$. Thus, we get the containment $\B_{\infty}(X) \subset \cdots \subset B_2(X) \subset B_1(X)$. 

Why should we care about this seemingly abstract space? All finite, two-layer neural networks belong to the Barron space with $\pi(w) = \frac{1}{m}\sum_{k=1}^m \delta(w- \hat{w}_k)$, and the Universal Approximation Theorem tells us that continuous functions can be approximated arbitrarily well by two-layer neural networks. Therefore, $\mathcal{B}_p(X)$ is dense in $C(X)$. 

Even more important, Barron spaces have a connection to reproducing kernel Hilbert spaces. For a fixed probability measure $\pi$, define 
\begin{align*}
    H_{\pi}(X) := \bigg\{  \int_{\mathbb{S}^d} a(w) \sigma(\langle w, x \rangle) d \pi(w) : \left\Vert f \right\Vert_{H_{\pi}} < \infty \bigg\}
\end{align*}
where
\begin{align*}
    \left\Vert f\right\Vert_{H_{\pi}} := \mathbb{E}[|a(w)|^2].
\end{align*}
Additionally, we recall that for a symmetric positive definite function $k: X \times X \mapsto \mathbb{R}$, the induces RKHS is the completion of $\{ \sum_i a_i k(x_i, x) \}$ with respect to the inner product $\langle k(x_i, \cdot), k(x_j, \cdot) \rangle_{H_k} = k(x_i, x_j)$. It has been shown that $H_{\pi} = H_{k_{\pi}}$, where the kernel $k_{\pi}$ is defined by 
\begin{align*}
    k_{\pi}(x, x) := \mathbb{E}_{\pi}[\sigma(\langle w, x \rangle)\sigma(\langle w, x' \rangle)].
\end{align*}
This means that the Barron space is equal to the union of reproducing kernel Hilbert spaces
\begin{align*}
    \mathcal{B}_2(X) = \underset{\pi \in P(\mathbb{S}^d)}{\bigcup} H_{\pi}(X).
\end{align*}

For a comparison of two-layer neural networks and kernel methods in terms of Barron spaces, see \cite{ma2018priori} pp. 6-8. In particular, Weinan and colleagues discuss the mathematical conditions under which the population risk for kernel methods is much larger than that for neural network models. The key distinction between neural networks and kernel methods is summarized as follows:\\ \enquote{The kernel method works with a specific RKHS with a particular choice of the kernel or the probability distribution $\pi$. In contrast, the neural network models work with the union of all these RKHS and select the kernel or the probability distribution adapted to the data} \cite{ma2018priori}.

\subsection{Main Results}

\begin{manualtheorem}{4.1}
Assume that the target function $f^* \in \mathcal{B}_2(X)$ and $\lambda \geq \lambda_n.$ Then for any $\delta > 0$, with probability at least $1 - \delta$ over the choice of training set $S$, we have
\begin{align*}
    \mathbb{E}_x|f(x; \hat{\theta}_{n, \lambda}) - f^*(x)|^2 \lesssim \frac{\gamma_2^2(f^*)}{m} + \lambda \hat{\gamma}_2(f^*) + \frac{1}{\sqrt{n}}(\hat{\gamma}_2(f^*) + \sqrt{\ln{(n/\delta)}}).
\end{align*}
\end{manualtheorem}

For the next result, the authors assume that the noise satisfies 
\begin{align*}
    \mathbb{P}[|\xi| > t] \leq c_0 e^{-\frac{t^2}{\sigma}}, \quad \forall t \geq \tau_0.
\end{align*}
where $c_0, \tau_0, \sigma$ are constants.
\begin{manualtheorem}{4.2}
Assume that the target function $f^* \in \mathcal{B}_2(X)$ and $\lambda \geq \lambda_n$. Then for any $\delta > 0$, with probability at least $1- \delta$ over the choice of the training set $S$, we have
\begin{align*}
    \mathbb{E}_x|f(x; \hat{\theta}_{n, \lambda}) - f^*(x)|^2 \lessim \frac{\gamma_2^2(f^*)}{m} + \lambda B_n \hat{\gamma}_2(f^*)\\
    + \frac{B_n^2}{\sqrt{n}}\left( \hat{\gamma}_2(f^*) + \sqrt{\ln(n/\delta)} \right)\\
    + \frac{B_n^2}{\sqrt{n}}\left( c_0 \sigma^2 + \sqrt{\frac{\mathbb{E}(\xi^2)}{n^{1/2}\lambda}} \right).
\end{align*}
\end{manualtheorem}

\bibliographystyle{siam}
\bibliography{References/biblio}

\end{document}