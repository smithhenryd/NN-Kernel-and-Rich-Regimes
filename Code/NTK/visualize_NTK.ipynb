{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vK4qqXp5qxhN"},"outputs":[],"source":["# Mount to my Google Drive\n","from google.colab import drive\n","import os\n","import pickle\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir(\"/content/drive/MyDrive/###\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahyW6M9erGkD"},"outputs":[],"source":["# Import necessary packages\n","import tensorflow as tf\n","import numpy as np\n","\n","# Import code for NTK callback function\n","from NTK_callback import NTKCallback\n","\n","# As well as the code for the linear regression model from Woodworth 2020\n","from model import Linear_Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_1UL8MeLQPm"},"outputs":[],"source":["# Write TensorFlow callback function for early stopping when a certain loss threshold is reached\n","class LossThreshold(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self, threshold, **kwargs):\n","        super(LossThreshold, self).__init__(**kwargs)\n","        self.threshold = threshold\n","\n","    def on_epoch_end(self, epoch, logs=None): \n","        \n","        loss = logs[\"loss\"]\n","        if loss <= self.threshold:\n","            self.model.stop_training = True"]},{"cell_type":"markdown","metadata":{"id":"I6dDZh0pvpxk"},"source":["# Initializing the Linear Model and Neural Tangent Kernel (NTK) Callback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"38aeuQBBsu4R"},"outputs":[],"source":["# Create our training data set\n","\n","# For reproducibility of results, set the random seed\n","tf.random.set_seed(500)\n","np.random.seed(500)\n","\n","# Dimension of input points\n","d = 20\n","# Number of training points\n","N = 10\n","\n","# Create the \\beta which parameterizes our linear regression model\n","# Here, we generate \\beta by taking each entry to be an i.i.d. Unif(0,1) random variable\n","beta = tf.random.uniform([d, 1], dtype=tf.float32)\n","\n","# As in Woodworth et al., suppose our training points are drawn from a d-dimensional\n","# standard multivariare normal distribution\n","train_x = np.random.multivariate_normal(np.zeros((d)), np.identity(d), size=N)\n","# NOTE: train_x here has dimension N x d\n","train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n","\n","# Compute the corresponding y-values\n","train_y = tf.reshape(tf.matmul(train_x, beta), (-1, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKl3DpvgvoZK"},"outputs":[],"source":["# Initialize the linear regression model\n","\n","# Initialization scale\n","alpha = 0.1\n","# Initialization shape\n","w0 = tf.ones([2*d, 1])\n","\n","# Create our model\n","model = Linear_Regression(w0, alpha=alpha)\n","# As well as the NTK callback object\n","ntk_callback = NTKCallback(train_x, step=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONlrRYSjweWZ"},"outputs":[],"source":["# Optimize the model using gradient descent \n","optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2)\n","\n","# We will use the mean squared error as our loss function L\n","# NOTE: this is the same loss that is used in Woodworth et al. (the squared loss), except scaled by a factor of 1/N (hence 'mean')\n","MSE = tf.keras.losses.MeanSquaredError()\n","model.compile(optimizer, loss=MSE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"127Yr3bMxeif"},"outputs":[],"source":["# Finally, fit out model\n","# We stop when our training loss reaches 10^{-4}\n","model.fit(train_x, train_y, epochs=10**4, verbose=1, callbacks=[ntk_callback, LossThreshold(1e-4)])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2g6mEGTvynEN"},"outputs":[],"source":["# We have indeed have all evaluations of the NTK during training\n","ntk_callback.NTK_evals[0:3]\n","# Notice that each list item is a 10 x 10 tensor representing the NTK evaluated on the training grid of N = 10 points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DeqWR_5Y0Nzy"},"outputs":[],"source":["# Number of NTK evaluations\n","len(ntk_callback.NTK_evals)"]},{"cell_type":"markdown","metadata":{"id":"5T2fMb6U0qcp"},"source":[" # Visualizing the NTK During Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63OCoU_f0m43"},"outputs":[],"source":["# Import plotting tools\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","\n","# And packages to create animation\n","import matplotlib.animation as animation\n","from IPython import display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSy7gugd1Ory"},"outputs":[],"source":["# Visualize the Neural Tangent Kernel of the model upon initialization\n","# This is equal to the NTK of the corresponding linearized model from Chizat et al. 2018\n","fig, ax = plt.subplots()\n","\n","ax.title.set_text(r\"$\\langle \\nabla_w f(w)(x), \\nabla_w f(w)(x) \\rangle$, epoch = 0\")\n","\n","# Labels for points\n","pt_labels = [r'$x_1$', r'$x_2$', r'$x_3$', r'$x_4$', r'$x_5$', r'$x_6$', r'$x_7$', r'$x_8$', r'$x_9$', r'$x_{10}$']\n","\n","# Plot the NTK evaluated at the training points\n","im = ax.imshow(ntk_callback.NTK_evals[0], cmap=\"Greens\", origin=\"lower\")\n","\n","# Add a colorbar to the right of the plot\n","divider = make_axes_locatable(ax)\n","cax = divider.append_axes('right', size='5%', pad=0.1)\n","fig.colorbar(im, cax=cax, orientation='vertical')\n","\n","\n","ax.set_xticks(range(0, 10))\n","ax.set_xticklabels(pt_labels)\n","ax.set_yticks(range(0, 10))\n","ax.set_yticklabels(pt_labels)\n","\n","fig.show()\n","fig.savefig(f'linearized_NTK_{alpha}.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HmAPM-d5CVF"},"outputs":[],"source":["# Parameters for our animation\n","num_frames = len(ntk_callback.NTK_evals)\n","fps = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACR5DKXKH6cC"},"outputs":[],"source":["# Minimum, maximum for colorbar\n","# We want the scale of the the plot to be constant; otherwise, we cannot visualize change throughout training\n","col_min = -1\n","col_max = 2.5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wL05jmDu4NWl"},"outputs":[],"source":["def update_plot(frame_number, zarray, plot):\n","  # Remove the previous plot\n","  plot[0].remove()\n","\n","  # And add the new one\n","  plot[0] = ax.imshow(ntk_callback.NTK_evals[frame_number], cmap=\"Greens\", origin=\"lower\", vmin=col_min, vmax=col_max)\n","  \n","  fig.show()\n","  return\n","\n","# Initialize the plot (using our previous code)\n","fig, ax = plt.subplots()\n","\n","plot = [ax.imshow(ntk_callback.NTK_evals[0], cmap=\"Greens\", origin=\"lower\", vmin=col_min, vmax=col_max)]\n","\n","ax.set_xticks(range(0, 10))\n","ax.set_xticklabels(pt_labels)\n","ax.set_yticks(range(0, 10))\n","ax.set_yticklabels(pt_labels)\n","\n","fig.show()\n","\n","zarray=1\n","\n","# Instantiate the animation object\n","ani = animation.FuncAnimation(fig, update_plot, num_frames, fargs=(zarray, plot), interval=1000/fps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bJPFzPb2V3F0"},"outputs":[],"source":["# Visualize the GIF in the notebook file\n","plt.rcParams['animation.html'] = 'html5'\n","ani"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8nFgjQ__Uog"},"outputs":[],"source":["# Install writer to save GIF\n","!apt-get update\n","!apt install imagemagick"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1OYALTbAIDo"},"outputs":[],"source":["# And save the GIF\n","ani.save(f'NTK_{alpha}.gif', writer='imagemagick', dpi=200)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMqV09i9dbtIku48jYLdrc1","collapsed_sections":[],"name":"visualize_NTK.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
