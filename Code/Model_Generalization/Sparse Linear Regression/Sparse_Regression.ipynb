{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22957,"status":"ok","timestamp":1650052269136,"user":{"displayName":"Henry Smith","userId":"08199033251338702938"},"user_tz":240},"id":"-NUngRRrlIYn","outputId":"eddbcff4-d261-4f23-81c0-49481b3e0363"},"outputs":[],"source":["# Mount to my Google Drive\n","from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir(\"/content/drive/MyDrive/###\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1650052269137,"user":{"displayName":"Henry Smith","userId":"08199033251338702938"},"user_tz":240},"id":"5qURk88ssj1e","outputId":"78381d3c-6583-445e-a165-6cb5eaf01757"},"outputs":[],"source":["# Check what GPU we are using\n","!nvidia-smi -L"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WwyfS-Dj01q"},"outputs":[],"source":["# Import necessary packages\n","import tensorflow as tf\n","import numpy as np\n","import pickle\n","import cvxpy as cvx\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hefcdsmkm-rm"},"outputs":[],"source":["# Import our code\n","from linear_regression_generalization import generate_sparse_dataset, train_linreg_network"]},{"cell_type":"markdown","metadata":{"id":"ED3mdebCnhKj"},"source":["# Model Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyCxw2saniB8"},"outputs":[],"source":["# Dimension of problem\n","d = 250\n","\n","# Sparsity of problem \n","# There are r nonzero coordinates of the beta vector, and they are the first five coordinates\n","r = 5\n","indices = range(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFZyCOJfnpAD"},"outputs":[],"source":["# Number of training points\n","N =  100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0lPGIxno2vc"},"outputs":[],"source":["# Generate our training dataset\n","train_data = generate_sparse_dataset(N, d, r, indices=indices, filename=f'training_data_sparse_regression_{N}.pk')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_HPWWejpyLQ"},"outputs":[],"source":["# And our test dataset\n","num_test = 100\n","test_data = generate_sparse_dataset(num_test, d, r, indices=indices, filename='test_data_sparse_regression.pk')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eH3uhy4zo5UO"},"outputs":[],"source":["# Initialization shape and scales for our NN\n","w0 = tf.ones([2*d, 1])\n","alphas = list(np.logspace(-3, 0, 20))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fmF5AZhn_PNp"},"outputs":[],"source":["# Load in previous data\n","with open(f'training_data_sparse_regression_{N}.pk', 'rb') as f:\n","    train_data = pickle.load(f)\n","\n","with open('test_data_sparse_regression.pk', 'rb') as f:\n","    test_data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCkiFW98JTEA"},"outputs":[],"source":["# Get the minimum l^1 norm solution for the training data\n","# Code from Woodworth et al. 2020\n","beta = cvx.Variable(train_data[0].shape[1])\n","prob = cvx.Problem(cvx.Minimize(cvx.norm(beta,1)), [train_data[0] @ beta == np.reshape(train_data[1], [-1])])\n","prob.solve()\n","solution_l1_norm = prob.value\n","beta_min_l1 = beta.value\n","beta_min_l1 = np.reshape(beta_min_l1, [-1, 1])"]},{"cell_type":"markdown","metadata":{"id":"K825G7eU4fDd"},"source":["# Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9910500,"status":"ok","timestamp":1645847596326,"user":{"displayName":"Henry Smith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMJZiKKD39VURreKQBzY_ip-4zlTy8DsH2cF19Qw=s64","userId":"08199033251338702938"},"user_tz":300},"id":"3LzbaRwJp8G6","outputId":"b2fe0eec-902f-4de1-b9af-fd932e974641"},"outputs":[],"source":["# Now let's train the models\n","output = train_linreg_network((train_data[0], train_data[1]), (test_data[0], test_data[1]), w0, alphas=alphas, lr=3*10**(-4))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LtgTcox8qp7G"},"outputs":[],"source":["# Save the output from training for posterity\n","with open(f'sparse_generalization_{N}.pk', 'wb') as f:\n","        pickle.dump(output, f)"]},{"cell_type":"markdown","metadata":{"id":"ClE-K6Cn4nHJ"},"source":["# Visualizing Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfQWCOsnAAug"},"outputs":[],"source":["# Import prior outputs\n","with open('sparse_generalization_50.pk', 'rb') as f:\n","    output50 = pickle.load(f)\n","\n","with open('sparse_generalization_100.pk', 'rb') as f:\n","    output100 = pickle.load(f)\n","\n","with open('sparse_generalization_200.pk', 'rb') as f:\n","    output200 = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkZtoSGYk-WW"},"outputs":[],"source":["# Boolean lists, True if gradient descent does converge within 10^4 iterations, False otherwise (no convergence)\n","converge50 = [i for i in range(len(output50[2])) if output50[2][i] < 9999]\n","converge100 = [i for i in range(len(output100[2])) if output100[2][i] < 9999]\n","converge200 = [i for i in range(len(output200[2])) if output200[2][i] < 9999]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":1705,"status":"ok","timestamp":1646319930995,"user":{"displayName":"Henry Smith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMJZiKKD39VURreKQBzY_ip-4zlTy8DsH2cF19Qw=s64","userId":"08199033251338702938"},"user_tz":300},"id":"zGH42p_GYeoG","outputId":"e433ce5c-9a34-486b-d9fc-ecb5a8628cf2"},"outputs":[],"source":["# Now plot the MSE on the test dataset against the initialization scale \\alpha for each of the models we trained\n","# A dashed line indicates that the model did not achieve the desired convergence\n","fig = plt.figure(0)\n","\n","MSE50 = [output50[1][i][-1] for i in range(len(alphas))]\n","MSE100 = [output100[1][i][-1] for i in range(len(alphas))]\n","MSE200 = [output200[1][i][-1] for i in range(len(alphas))]\n","\n","# plt.plot(alphas[0:min(converge50)], MSE50[0:min(converge50)], color=\"#2ca02c\", linestyle=\"dashed\")\n","# plt.plot(alphas[(min(converge50)-1):], MSE100[(min(converge50)-1):], color=\"#2ca02c\", label=\"50\")\n","\n","plt.plot(alphas[0:min(converge100)], MSE100[0:min(converge100)], color=\"#1f77b4\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge100)-1):], MSE100[(min(converge100)-1):], color=\"#1f77b4\", label=\"100\")\n","\n","plt.plot(alphas[0:min(converge200)], MSE200[0:min(converge200)], color=\"#ff7f0e\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge200)-1):], MSE200[(min(converge200)-1):], color=\"#ff7f0e\",label=\"200\")\n","\n","plt.xscale(\"log\")\n","plt.yscale(\"log\")\n","plt.legend()\n","\n","plt.xlabel(r\"Initialization Scale ($\\alpha$)\")\n","plt.ylabel(\"Test MSE\")\n","\n","fig.show()\n","fig.savefig('visualize_test_MSE_log.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":2404,"status":"ok","timestamp":1650052311370,"user":{"displayName":"Henry Smith","userId":"08199033251338702938"},"user_tz":240},"id":"FHDr3tZYdj17","outputId":"f120a5c9-2438-4ed6-eee5-13ed1d699c2b"},"outputs":[],"source":["# Rather than looking at the MSE of the model on the test dataset, we can instead consider the \\ell^2 distance between the learned and true \n","# beta vector, as Woodworth and colleagues do in their paper. This is because, by the Riesz representation theorem, the model evaluated at any input x\n","# is completely detemined by \\beta according to the \\ell^2 inner product of \\beta and x.  \n","fig = plt.figure(0)\n","\n","beta_0 = train_data[2]\n","\n","l2error50 = [np.linalg.norm((np.square(output50[3][i].numpy())[0:d] - np.square(output50[3][i].numpy())[d:]) - beta_0, ord=2)**2 for i in range(len(alphas))]\n","l2error100 = [np.linalg.norm((np.square(output100[3][i].numpy())[0:d] - np.square(output100[3][i].numpy())[d:]) - beta_0, ord=2)**2 for i in range(len(alphas))]\n","l2error200 = [np.linalg.norm((np.square(output200[3][i].numpy())[0:d] - np.square(output200[3][i].numpy())[d:]) - beta_0, ord=2)**2 for i in range(len(alphas))]\n","\n","plt.plot(alphas[0:min(converge50)], l2error50[0:min(converge50)], color=\"#2ca02c\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge50)-1):], l2error50[(min(converge50)-1):], color=\"#2ca02c\", label=\"50\")\n","\n","plt.plot(alphas[0:min(converge100)], l2error100[0:min(converge100)], color=\"#1f77b4\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge100)-1):], l2error100[(min(converge100)-1):], color=\"#1f77b4\", label=\"100\")\n","\n","plt.plot(alphas[0:min(converge200)], l2error200[0:min(converge200)], color=\"#ff7f0e\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge200)-1):], l2error200[(min(converge200)-1):], color=\"#ff7f0e\",label=\"200\")\n","\n","plt.xscale(\"log\")\n","plt.yscale(\"log\")\n","plt.legend()\n","\n","plt.xlabel(r\"Initialization Scale ($\\alpha$)\")\n","plt.ylabel(r\"$\\ell^2$ Population Error  $\\Vert \\beta - \\beta^* \\Vert_2^2$\")\n","\n","fig.show()\n","fig.savefig('12_population_error_log_all.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":1646,"status":"ok","timestamp":1649991244416,"user":{"displayName":"Henry Smith","userId":"08199033251338702938"},"user_tz":240},"id":"4Mpq1-wrx1oH","outputId":"c50e4853-b66e-4c82-975d-3b4dfad82f73"},"outputs":[],"source":["# Likewise, we look at the \\ell^1 distance between the learned and true beta vector as a measure of the population error\n","\n","l1error50 = [np.linalg.norm((tf.math.square(i)[0:d] - tf.math.square(i)[d:]) - beta_0, ord=1) for i in output50[3]]\n","l1error100 = [np.linalg.norm((tf.math.square(i)[0:d] - tf.math.square(i)[d:]) - beta_0, ord=1) for i in output100[3]]\n","l1error200 = [np.linalg.norm((tf.math.square(i)[0:d] - tf.math.square(i)[d:]) - beta_0, ord=1) for i in output200[3]]\n","\n","fig = plt.figure(1)\n","\n","plt.plot(alphas[0:min(converge50)], l1error50[0:min(converge50)], color=\"#2ca02c\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge50)-1):], l1error50[(min(converge50)-1):], color=\"#2ca02c\", label=\"50\")\n","\n","plt.plot(alphas[0:min(converge100)], l1error100[0:min(converge100)], color=\"#1f77b4\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge100)-1):], l1error100[(min(converge100)-1):], color=\"#1f77b4\", label=\"100\")\n","\n","plt.plot(alphas[0:min(converge200)], l1error200[0:min(converge200)], color=\"#ff7f0e\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge200)-1):], l1error200[(min(converge200)-1):], color=\"#ff7f0e\",label=\"200\")\n","\n","\n","plt.xscale(\"log\")\n","plt.yscale(\"log\")\n","plt.legend()\n","\n","plt.xlabel(r\"Initialization Scale ($\\alpha$)\")\n","plt.ylabel(r\"$\\ell^1$ Population Error  $\\Vert \\beta - \\beta_{w} \\Vert_1$\")\n","plt.show()\n","fig.savefig('11_population_error_log_all.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":286},"executionInfo":{"elapsed":1860,"status":"ok","timestamp":1649992741975,"user":{"displayName":"Henry Smith","userId":"08199033251338702938"},"user_tz":240},"id":"TlVfhcs3KqHi","outputId":"e81fecee-d3b8-4d8c-ded0-9001abb2c9e8"},"outputs":[],"source":["# Plot the \\ell^1 distance between the learned beta vector and the minimum \\ell^1 solution to the system X\\beta = y\n","\n","l1error50 = [np.linalg.norm((tf.math.square(i)[0:d] - tf.math.square(i)[d:]) - beta_min_l1, ord=1) for i in output50[3]]\n","l1error100 = [np.linalg.norm((tf.math.square(i)[0:d] - tf.math.square(i)[d:]) - beta_min_l1, ord=1) for i in output100[3]]\n","l1error200 = [np.linalg.norm((tf.math.square(i)[0:d] - tf.math.square(i)[d:]) - beta_min_l1, ord=1) for i in output200[3]]\n","\n","fig = plt.figure(1)\n","\n","plt.plot(alphas[0:min(converge50)], l1error50[0:min(converge50)], color=\"#2ca02c\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge50)-1):], l1error50[(min(converge50)-1):], color=\"#2ca02c\", label=\"50\")\n","\n","plt.plot(alphas[0:min(converge100)], l1error100[0:min(converge100)], color=\"#1f77b4\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge100)-1):], l1error100[(min(converge100)-1):], color=\"#1f77b4\", label=\"100\")\n","\n","plt.plot(alphas[0:min(converge200)], l1error200[0:min(converge200)], color=\"#ff7f0e\", linestyle=\"dashed\")\n","plt.plot(alphas[(min(converge200)-1):], l1error200[(min(converge200)-1):], color=\"#ff7f0e\",label=\"200\")\n","\n","\n","plt.xscale(\"log\")\n","plt.yscale(\"log\")\n","plt.legend()\n","\n","plt.xlabel(r\"Initialization Scale ($\\alpha$)\")\n","plt.ylabel(r\"Excess $\\ell^1$ Norm  $\\Vert \\beta - \\beta^{\\ell^1} \\Vert_1$\")\n","plt.show()\n","fig.savefig('excess_l1_log_all.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":2205,"status":"ok","timestamp":1646320116786,"user":{"displayName":"Henry Smith","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMJZiKKD39VURreKQBzY_ip-4zlTy8DsH2cF19Qw=s64","userId":"08199033251338702938"},"user_tz":300},"id":"B_HHXF3n2fDB","outputId":"e8a0061d-f19d-4828-a550-854f2918b25a"},"outputs":[],"source":["# Plot the number of iterations necessary for gradient descent to converge below 10^(-4),\n","# our predetmined threshold on the training loss\n","fig = plt.figure(2)\n","\n","plt.plot(alphas, output50[2], color=\"#2ca02c\", label=\"50\")\n","\n","plt.plot(alphas, output100[2], color=\"#1f77b4\", label=\"100\")\n","\n","plt.plot(alphas, output200[2], color=\"#ff7f0e\", label=\"200\")\n","\n","plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n","plt.xscale(\"log\")\n","plt.legend()\n","\n","plt.xlabel(r\"Initialization Scale ($\\alpha$)\")\n","plt.ylabel(\"Number of Training Epochs\")\n","\n","fig.show()\n","fig.savefig('interations_to_convergence_all.png', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"elapsed":1086,"status":"ok","timestamp":1650052336582,"user":{"displayName":"Henry Smith","userId":"08199033251338702938"},"user_tz":240},"id":"MXh9p-BVH_qf","outputId":"980c1bda-7e5c-4de3-e354-4b13691ff334"},"outputs":[],"source":["# Plot the first 10 components of the empirical solution vector \\beta against those of the \"true\" solution vector \\beta^*\n","fig = plt.figure(3)\n","\n","plt.plot(np.arange(1, 11), beta_0[0:10], '.', label=r\"$\\beta_w$\")\n","plt.plot(np.arange(1, 11), (tf.math.square(output100[3][0])[0:d] - tf.math.square(output100[3][0])[d:])[0:10], '.', label=r\"$\\beta_{\\alpha = 10^{-3}}$\")\n","\n","plt.legend()\n","fig.savefig('visualize_solution_vec_0.001.png', dpi=300)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPz3XA6EBV1oeFBafeltCe4","collapsed_sections":[],"name":"Sparse_Regression.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
