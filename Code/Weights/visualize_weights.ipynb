{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L26xE13oJvKx"
      },
      "outputs": [],
      "source": [
        "# Mount to my Google Drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pickle\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(\"/content/drive/MyDrive/###\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvpIFuTYQm33"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import linear regression code \n",
        "from model import Linear_Regression\n",
        "from linearized_model import Linearized_Model, LinearizedCallback\n",
        "from weights_callback import WeightsCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S4B5oRVQc7r"
      },
      "source": [
        "# Generating Training Data and Automatic Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZlXt6GcJ8F2"
      },
      "outputs": [],
      "source": [
        "# Number of training points\n",
        "N = 10\n",
        "# Dimension of training points \n",
        "d = 20\n",
        "\n",
        "# Initialization shape and scale\n",
        "alpha = 1\n",
        "w0 = tf.ones([2*d, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mHkGtidQuMG"
      },
      "outputs": [],
      "source": [
        "# Generate beta by taking each coordinate to be an iid Unif(0,1) random variable\n",
        "beta = tf.random.uniform([d, 1], dtype=tf.float32)\n",
        "\n",
        "# As in Woodworth et al., suppose our training points are drawn from a d-dimensional\n",
        "# standard multivariate normal distribution\n",
        "train_x = np.random.multivariate_normal(np.zeros((d)), np.identity(d), size=N)\n",
        "train_x = tf.convert_to_tensor(train_x, dtype=tf.float32)\n",
        "\n",
        "# Compute the corresponding y-values\n",
        "train_y = tf.reshape(tf.matmul(train_x, beta), (-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45mmISbQast5"
      },
      "outputs": [],
      "source": [
        "# Save the training data for future use\n",
        "x_list = list(train_x.numpy())\n",
        "y_list = list(train_y.numpy())\n",
        "\n",
        "with open('train_data.pkl', 'wb') as f:\n",
        "  pickle.dump([x_list, y_list, beta], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4ws8bj7bfum"
      },
      "outputs": [],
      "source": [
        "# Load in existing training data\n",
        "with open('train_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "train_x = tf.convert_to_tensor(data[0], dtype=tf.float32)\n",
        "train_y = tf.convert_to_tensor(data[1], dtype=tf.float32)\n",
        "beta = tf.convert_to_tensor(data[2], dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d535GO8QQ_-F"
      },
      "outputs": [],
      "source": [
        "# Initialize our models\n",
        "# Linear regression:\n",
        "linreg = Linear_Regression(w0, alpha=alpha)\n",
        "\n",
        "# Linearized model:\n",
        "linreg_const = Linear_Regression(w0, alpha=alpha)\n",
        "# Recall that the linear regression model is 2-homogeneous\n",
        "linearized = Linearized_Model(linreg_const, train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BUGHqJhQ4BV"
      },
      "outputs": [],
      "source": [
        "# Parameters for network training\n",
        "epochs = 1e3\n",
        "lr = 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY3NVJ8DSwC0"
      },
      "outputs": [],
      "source": [
        "# Optimize each model using gradient descent\n",
        "optimizer_linreg = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "optimizer_linearized = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "\n",
        "# With loss function equal to the mean-squared error\n",
        "MSE = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqfZcvbIS2Qa"
      },
      "outputs": [],
      "source": [
        "# Instantiate callback objects for our models\n",
        "weights_linear = WeightsCallback(10)\n",
        "weights_linearized = WeightsCallback(10)\n",
        "linearized_callback = LinearizedCallback(linreg_const)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzdMUkGwTJqy"
      },
      "outputs": [],
      "source": [
        "# First, compile and fit the linear regression model\n",
        "linreg.compile(optimizer_linreg, loss=MSE)\n",
        "linreg.fit(train_x, train_y, epochs=epochs, verbose=1, callbacks=[weights_linear])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLtveZs1T_bT"
      },
      "outputs": [],
      "source": [
        "# As well as the linearized model\n",
        "linearized.compile(optimizer_linearized, loss=MSE)\n",
        "linearized.fit(train_x, train_y, epochs=epochs, verbose=1, callbacks=[weights_linearized, linearized_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNHhLAkBUe4_"
      },
      "outputs": [],
      "source": [
        "with open('linearized_weights.pkl', 'wb') as f:\n",
        "  pickle.dump(weights_linear.weight_evals, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoj-vZp_L_NR"
      },
      "source": [
        "# Manual Model Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbXboNQ3N_rP"
      },
      "source": [
        "Code adapted from [TensorFlow custom training walkthrough](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZbKqfcoOcQM"
      },
      "outputs": [],
      "source": [
        "# We define our model using the same training data as above\n",
        "tf.print(f\"training data x: {tf.shape(train_x)}\")\n",
        "tf.print(f\"training data y: {tf.shape(train_y)}\")\n",
        "tf.print(f\"y - beta*x:\\n{train_y - tf.matmul(train_x, beta)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNiB4W0nQTrc"
      },
      "outputs": [],
      "source": [
        "### REMOVE THE BELOW LINE TO PRINT OUTPUT\n",
        "%%capture\n",
        "\n",
        "# Initialization scale and shape\n",
        "alpha = 10\n",
        "w0 = tf.ones([2*tf.shape(train_x)[1], 1])\n",
        "\n",
        "# Instantiate the linear regression model\n",
        "linreg = Linear_Regression(w0, alpha)\n",
        "\n",
        "# Instantiate the linearized model\n",
        "linreg_const = Linear_Regression(w0, alpha)\n",
        "linearized = Linearized_Model(linreg_const, train_x, alpha, hom=2)\n",
        "\n",
        "tf.print(f\"w0:\\n{linearized.linearized_layer_1.init}\")\n",
        "tf.print(f\"gradients:\\n{linearized.linearized_layer_1.grads}\")\n",
        "tf.print(f\"bias:\\n{linearized.linearized_layer_1.bias}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRLgL53fMB-6"
      },
      "outputs": [],
      "source": [
        "# Choose learning rate and number of epochs\n",
        "lr = 1e-3\n",
        "num_epochs = int(1e4)\n",
        "\n",
        "# Use this learning rate to create our optimizer objects\n",
        "optimizer_linreg = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "optimizer_linearized = tf.keras.optimizers.SGD(learning_rate=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08B54IgqMsSo"
      },
      "outputs": [],
      "source": [
        "# Define our loss function\n",
        "MSE = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Define a function to compute the gradient of the model at each training epoch\n",
        "# We use use the tf.function decorator for faster training \n",
        "@tf.function\n",
        "def grad(model, x_train, y_train):\n",
        "  \n",
        "  N = int(tf.shape(x_train)[0])\n",
        "  p = int(tf.shape(tf.reshape(model.trainable_weights, [-1, 1]))[0])\n",
        "\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # evaluate the loss of the model at the training points\n",
        "    eval = model(x_train)\n",
        "    loss = MSE(y_train, eval)\n",
        "\n",
        "  # Return the training loss, the gradient of the model with respect to the training weights (averaged over all training points x),\n",
        "  # and the jacobian of the model output with respect to the training points\n",
        "  return loss, tape.gradient(loss, model.trainable_variables), tf.reshape(tape.jacobian(eval, model.trainable_variables), [N,p])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIO20LVDPVNV"
      },
      "outputs": [],
      "source": [
        "# Try taking a single training step\n",
        "\n",
        "print(\"Linear Regression:\")\n",
        "loss_val, linreg_grads, jacobian  = grad(linreg, train_x, train_y)\n",
        "\n",
        "# Loss before first step\n",
        "print(\"Step: {}, Initial Loss: {}\".format(optimizer_linreg.iterations.numpy(),\n",
        "                                          loss_val.numpy()))\n",
        "\n",
        "# Loss after first step\n",
        "optimizer_linreg.apply_gradients(zip(linreg_grads, linreg.trainable_variables))\n",
        "print(\"Step: {}, Loss: {}\".format(optimizer_linreg.iterations.numpy(),\n",
        "                                          MSE(train_y, linreg(train_x)).numpy()))\n",
        "\n",
        "# For the linearized model\n",
        "print(\"\\nLinearized:\")\n",
        "loss_val, linearized_grads, jacobian  = grad(linearized, train_x, train_y)\n",
        "\n",
        "# Loss before first step\n",
        "print(\"Step: {}, Initial Loss: {}\".format(optimizer_linearized.iterations.numpy(),\n",
        "                                          loss_val.numpy()))\n",
        "\n",
        "# Loss after first step\n",
        "optimizer_linearized.apply_gradients(zip(linearized_grads, linearized.trainable_variables))\n",
        "print(\"Step: {}, Loss: {}\".format(optimizer_linearized.iterations.numpy(),\n",
        "                                          MSE(train_y, linearized(train_x)).numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxgsaVq8U8dC"
      },
      "outputs": [],
      "source": [
        "# The gradient of the linearized model at the training model should remain constant throughout training (by definition of the linearized model)\n",
        "tf.norm(linearized.linearized_layer_1.grads - jacobian, ord=np.inf, axis=None).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNdgfMkDUlKh"
      },
      "outputs": [],
      "source": [
        "# Now, train for num_epochs epochs\n",
        "\n",
        "# List of model weights\n",
        "linreg_weights = []\n",
        "linearized_weights = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "\n",
        "  # Compute the loss, gradient of the model output\n",
        "  linreg_loss, linreg_grads, linreg_jacobian = grad(linreg, train_x, train_y)\n",
        "  linearized_loss, linearized_grads, linearized_jacobian = grad(linearized, train_x, train_y)\n",
        "\n",
        "  # Every 10 iterations, print the loss and store the models' weights\n",
        "  if not i % 10:\n",
        "    print(\"Step: {}\".format(optimizer_linearized.iterations.numpy()))\n",
        "    print(\"(Linear regression) Loss: {}\".format(MSE(train_y, linreg(train_x))))\n",
        "    print(\"(Linearized) Loss: {}\\n\".format(MSE(train_y, linearized(train_x))))\n",
        "\n",
        "    linreg_weights.append(tf.reshape(linreg.trainable_variables, [-1,1]))\n",
        "    linearized_weights.append(tf.reshape(linearized.trainable_variables, [-1,1]))\n",
        "  \n",
        "\n",
        "  # Update the model by taking the gradient descent step \n",
        "  optimizer_linreg.apply_gradients(zip(linreg_grads,linreg.trainable_variables))\n",
        "  optimizer_linearized.apply_gradients(zip(linearized_grads,linearized.trainable_variables))\n",
        "\n",
        "# Save weights at the end of training\n",
        "linreg_weights.append(tf.reshape(linreg.trainable_variables, [-1,1]))\n",
        "linearized_weights.append(tf.reshape(linearized.trainable_variables, [-1,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUdyRlmNh5O5"
      },
      "outputs": [],
      "source": [
        "# Save the loss arrays\n",
        "with open('linreg_loss_5.pkl', 'wb') as f:\n",
        "  pickle.dump(linreg_weights, f)\n",
        "\n",
        "with open('linearized_loss_5.pkl', 'wb') as g:\n",
        "  pickle.dump(linearized_weights, g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yw697A-TQGr"
      },
      "source": [
        "# Visualizing the Weights During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXEK6bcxLpQj"
      },
      "outputs": [],
      "source": [
        "# Load in the training weights for the models trained with each corresponding alpha\n",
        "\n",
        "linreg_weights = []\n",
        "linearized_weights = []\n",
        "\n",
        "alphas = [\"0.05\", \"0.1\", \"0.5\", \"1\", \"5\"]\n",
        "\n",
        "for i in alphas:\n",
        "  \n",
        "  with open('linreg_loss_'+f'{i}'+'.pkl', 'rb') as f:\n",
        "    linreg_weights.append(pickle.load(f))\n",
        "  \n",
        "  with open('linearized_loss_'+f'{i}'+'.pkl', 'rb') as g:\n",
        "    linearized_weights.append(pickle.load(g))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xPvd2q6hbIs"
      },
      "outputs": [],
      "source": [
        "# Plot the \\ell_2 norm of the difference between the nonlinear and linearized model weights throughout training\n",
        "# As \\alpha \\rightarrow \\infty, we should observe that this norm goes to 0 for all times t\n",
        "# See Chizat et al. 2018 Theorem 2.2\n",
        "fig = plt.figure(0)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  diffs = [tf.norm(linreg_weights[i][j] - linearized_weights[i][j], ord=2) for j in range(len(linreg_weights[i]))]\n",
        "\n",
        "  plt.plot(10*np.arange(len(linreg_weights[i])), diffs, label=f\"{alphas[i]}\")\n",
        "\n",
        "plt.xlabel(r\"Epoch $t$\")\n",
        "plt.ylabel(r\"$\\left\\Vert w(t) - \\bar{w}(t) \\right\\Vert_2$\")\n",
        "plt.legend()\n",
        "\n",
        "fig.show()\n",
        "fig.savefig('visualize_weights.png', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-UYTzw2Nt6Z"
      },
      "outputs": [],
      "source": [
        "# Similarly, we should observe that the \\ell_2 norm of the difference between the initialization w(0) and the model weights w(t)\n",
        "# of the nonlinear model goes to 0 as \\alpha \\rightarrow \\infty for all times t during training\n",
        "fig = plt.figure(1)\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  delta = [tf.norm(linreg_weights[i][j] - linreg_weights[i][0], ord=2) for j in range(len(linreg_weights[i]))]\n",
        "\n",
        "  plt.plot(10*np.arange(len(linreg_weights[i])), delta, label=f\"{alphas[i]}\")\n",
        "\n",
        "plt.xlabel(r\"Epoch $t$\")\n",
        "plt.ylabel(r\"$\\left\\Vert w(t) - w(0) \\right\\Vert_2$\")\n",
        "plt.legend()\n",
        "\n",
        "fig.show()\n",
        "fig.savefig('visualize_change.png', dpi=300)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "visualize_weights.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
