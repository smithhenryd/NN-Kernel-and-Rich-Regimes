\documentclass{article}

\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% Formatting and images 
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{soul}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{indentfirst}
\usepackage{csquotes}

%% Packages for mathematical typesetting
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{gensymb}
\usepackage{pgf}
\usepackage{comment}
\usepackage{float}
\usepackage{blindtext}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{array}

% Title content
\title{\textbf{S\&DS Senior Project Proposal: \\Kernel and Rich Regimes in Deep Learning}}
\author[]{Henry Smith \\
Advised by Professor Harrison Zhou}
\affil[]{\normalsize Yale University}
\date{\today}

\begin{document}

\maketitle

\section{Problem Overview}
Much of the popularity and success of contemporary deep learning models can be attributed to their ability to generalize well when presented with unseen data. Indeed, researchers have constructed convolutional neural networks (CNNs) that achieve $< 1$\% test error on the MNIST dataset, outpacing other popular machine learning classifiers (k-nearest neighbors, support vector machines, etc.). Despite the widespread application of neural networks, practitioners remain largely unsure about the mathematical justification for why they generalize so well. 

Broadly speaking, neural networks are highly overparameterized; that is, the dimension of the parameter space $\boldsymbol{w} \in \mathbb{R}^p$ is much larger than the number of observations $N$ with which the network is trained. As a result, the loss function $L$—which measures the misfit of the model for the training data $(\boldsymbol{x}_1, y_1), \ldots, (\boldsymbol{x}_N, y_N)$\\—often has many global minima where zero training error is achieved. As keenly noted in \cite{woodworth2020kernel}, though, not all of these minima result in a model $f$ which generalizes equally well.

The goal of the present project is to understand the conditions under which training an overparameterized neural network will lead to a solution that generalizes well. Both \cite{chizat2018lazy} and \cite{woodworth2020kernel} investigate this problem and establish a connection between a network's initialization and the solution reached by the gradient flow dynamics. In particular, consider a network $f$ initialized with weight vector $\alpha \boldsymbol{w}_0$ and trained with a fixed $N$-many observations. Here, $\alpha > 0$ serves as a scaling parameter and $\boldsymbol{w}_0$ is the \enquote{shape} of the initialization. Under certain conditions on the model $f$ and loss function $L$, it is observed that as $\alpha \rightarrow \infty$, the gradient flow solution generalizes poorly on test data. \cite{chizat2018lazy} and \cite{woodworth2020kernel} refer to this limit as the \enquote{lazy} or \enquote{kernel} regime. Conversely, as $\alpha \rightarrow 0$ we observe the \enquote{rich limit} in which the gradient flow solution generalizes well. In the common setting of a neural network with weights initialized $\boldsymbol{w}_i \overset{i.i.d.}{\sim} \mathcal{W}$, one would analogously look at $\text{Var}(\mathcal{W})$, with the kernel regime induced as $\text{Var}(\mathcal{W}) \rightarrow \infty$ and the rich limit as $\text{Var}(\mathcal{W}) \rightarrow 0$.

Naturally, one might ask why one cannot take the initialization scale $\alpha$ to be arbitrarily small so as to achieve a model with the desired test error. It is important to note, though, that for many problems, $\boldsymbol{w} = \vec{0}$ is a saddle point for the objective $L(f(\boldsymbol{w}))$, and so taking $\alpha$ too small makes the optimization problem computationally intractable. Consequently, \cite{woodworth2020kernel} discusses the ideal scenario in which one should choose an initialization scale that acts \enquote{at the edge of the rich limit.} That is, one should choose $\alpha$ small enough so that the solution realized by gradient flow generalizes well (i.e. is in the \enquote{rich} training regime), but is also not so small that optimization is infeasible. 

Woodworth and colleagues in \cite{woodworth2020kernel} posit that the contemporary successes in deep learning are a result of models that operate in the rich regime. The implications of this statement are self-evident, and they warrant further computational and theoretical study of the kernel and rich limits.

\section{Problem Statement \& Methodology}

\subsection{Observing the Kernel Regime}
The first portion of the project will focus on visualizing and reproducing the results from \cite{chizat2018lazy} and \cite{woodworth2020kernel}. Underlying the previously mentioned kernel and rich regimes, there is rich literature discussing what happens mathematically as these limits are reached. 

Without digging too deep into the weeds, suppose that model $f$ maps from the parameter space $\mathbb{R}^p$ to Hilbert space $\mathcal{F}$ that represents  the space of possible functions our neural network can represent.\footnote{A common example is $\mathcal{F} = L^2(\rho_x, \mathbb{R}^k)$, where $\rho_x$ is the distribution of training observations.} Further, suppose that $f$ is a $D$-homogeneous function, meaning $f(\alpha \boldsymbol{w}) = \alpha^D f(\boldsymbol{w})$ for any $\alpha \geq 0$. Under certain technical conditions on model $f$ and loss $L$ detailed in \cite{chizat2018lazy}, then as $\alpha \rightarrow \infty$ in the initialization $\boldsymbol{w}(0) = \alpha \boldsymbol{w}_0$, we have that $f$ approaches the affine model 
\begin{align}\label{linearizedmodel}
    \bar{f}(\boldsymbol{w}) := \alpha^D f(\boldsymbol{w}(0)) + \left\langle \nabla f(\boldsymbol{w}(0)), \boldsymbol{w} - \boldsymbol{w}(0) \right\rangle
\end{align}
in the Hilbert space norm for all times $t \geq 0$ in the gradient flow dynamics \cite{chizat2018lazy}. That is, in the kernel regime, training our model $f$ is exactly equivalent to training an affine model $\bar{f}$. Notice that for this affine model $\bar{f}$, the gradient $\nabla \bar{f}(\bar{\boldsymbol{w}}(t)) = \nabla f(\bar{\boldsymbol{w}}(0))$ for all times $t \geq 0$, assuming that $\bar{\boldsymbol{w}}(t)$ evolves according to the gradient flow dynamics with $\bar{\boldsymbol{w}}(0) = \alpha \boldsymbol{w}_0$.

Therefore, we can look at the \textit{neural tangent kernel} (NTK)
\begin{align*}
    K_{\boldsymbol{w}(t)}(\boldsymbol{x}, \boldsymbol{x}') = \langle \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t))(\boldsymbol{x}), \nabla_{\boldsymbol{w}} f(\boldsymbol{w}(t))(\boldsymbol{x}') \rangle
\end{align*}
as a proxy for the kernel regime \cite{jacot2018neural}. Specifically, as a model approaches the kernel limit, we should observe that the NTK remains almost constant throughout training. 

This bring me to my first goal for the project: to visualize how the NTK evolves thoughout training for the rich versus kernel regimes. To do so, I will generate a training dataset in Python which corresponds to an appropriate model. Of particular interest is the linear regression model considered in \cite{woodworth2020kernel}:
\begin{align}\label{linrreg}
    f(\boldsymbol{w})(\boldsymbol{x}) = \sum_{i=1}^d (\boldsymbol{w}_{+, i}^2 - \boldsymbol{w}_{-, i}^2)\boldsymbol{x}_i = \langle \boldsymbol{\beta}_{\boldsymbol{w}}, \boldsymbol{x} \rangle, \quad \boldsymbol{w} = \begin{bmatrix}
                        \boldsymbol{w}_+ \\
                        \boldsymbol{w}_-
                        \end{bmatrix} \in \mathbb{R}^{2d},
    \quad \boldsymbol{\beta}_{\boldsymbol{w}} = \boldsymbol{w}_+^2 - \boldsymbol{w}_-^2.
\end{align}
I plan to use TensorFlow, a deep learning library for Python, to construct and optimize the model with weight initialization $\alpha \boldsymbol{w}_0$. Training will be performed using gradient descent, which can be thought of as a discretization of the gradient flow dynamics with some stepsize $\eta > 0$. During each $n$th iteration of the training procedure, I will evaluate the NTK at the training points $\boldsymbol{x}_1, \ldots, \boldsymbol{x}_N$. This will allow us to visualize the changing of the NTK as we interpolate between the kernel and rich regimes.

In addition to the neural tangent kernel, we will also study how the network weights $\boldsymbol{w}$ evolve throughout training. From our previous discussion of the kernel regime, we know that as $\alpha \rightarrow \infty$, the gradient flow of $L(f(\boldsymbol{w}))$ approaches that of $L(\bar{f}(\boldsymbol{w}))$. Accordingly, we can observe the $\ell_2$ distance between the weights of the model, $\boldsymbol{w}(t)$, and those of the affine model (\ref{linearizedmodel}), $\bar{\boldsymbol{w}}(t)$, for various values of $\alpha$ during training. We will do so using the linear regression model (\ref{linrreg}) implemented in Python as described in the previous paragraph.

\subsection{The Kernel Regime \& Model Generalization}
Moving away from the mathematics motivating the kernel regime, we will look at the generalization of neural networks trained under the kernel and rich regimes. From the prior section, we know that for a fixed number of training observations $N$, overparameterized networks trained in the rich regime generally achieve smaller test errors than those trained in the kernel regime. We will empirically demonstrate that this observation holds for the linear regression model (\ref{linrreg}), which was previously illustrated by Woodworth and colleagues in \cite{woodworth2020kernel}. Differing from Woodworth's analysis, though, we will also consider the time necessary to train a given model. That is, we will plot the number of iterations $n_{\alpha, \kappa}$ of gradient descent necessary to attain training error below some threshold $\kappa$ as a function of the initialization scale $\alpha$. We expect that as we get closer and closer to the rich limit $\alpha \rightarrow 0$, the resulting models will become more and more costly to train.

Moving past the example of linear regression, we hope to consider other interesting models, such as the [overparameterized] logistic regression model studied in \cite{wei2019regularization}. Wei and colleagues establish the mathematical result that training the logistic model in the kernel regime leads to poor generalization, which can be mitigated with the addition of a regularization term. It would be especially appealing to investigate whether by training the unregularized logistic regression model in the rich regime instead, we could achieve comparable test error to that of the regularized model. Of further interest are sparse problems, where the underlying model $f$ from which the data $\{(\boldsymbol{x}_i, y_i)\}_{i=1}^N$ is generated exhibits sparsity in its parameters \cite{ma2018priori}. We would expect that, since the parameters $\boldsymbol{w}$ change less during training when in the kernel regime than in the rich regime (see \cite{chizat2018lazy}), we would observe much smaller test error in the rich regime. 

\section{Deliverables}
\begin{enumerate}
    \item Multiple deep learning models implemented in Python with TensorFlow
    \item Python visualization of the neural tangent kernel $K_{\boldsymbol{w}(t)}$ while training in the rich and kernel regimes
    \item Plot of the $\ell_2$ distance between the weights of the original model, $\boldsymbol{w}(t)$, and those of the affine model (\ref{linearizedmodel}), $\bar{\boldsymbol{w}}(t)$, throughout training with gradient descent for various $\alpha$
    \item Graphs of test error versus initialization scale for various overparameterized models (linear regression (\ref{linrreg}), logistic regression, sparse)
    \item Analysis of the relationship between the initialization scale $\alpha$ and number of training iterations $n_{\alpha, \kappa}$ for the linear regression model
\end{enumerate}

\section{Tentative Timeline}
\noindent\begin{tabular*}{\textwidth}{@{\hspace{\labelwidth}\llap{}\hspace{\labelsep}}p{3in}@{\extracolsep{\fill}}r}
    Literature review, discussion with Professor Zhou & January 25 -- February 6\\[\itemsep]
    Code linear regression (\ref{linrreg}) and affine (\ref{linearizedmodel}) models & January 30 -- February 20\\[\itemsep]
    Visualize the NTK, weights during training & February 20 -- March 6\\[\itemsep]
    Examine test loss versus initialization scale for (\ref{linrreg}) & March 6 -- March 20\\[\itemsep]
    Code logistic regression model, examine test loss & March 20 -- April 10\\[\itemsep]
    Code sparse model, examine test loss & April 10 -- April 24\\[\itemsep]
    Prepare final report, poster & April 17 -- May 5\\[\itemsep]
\end{tabular*}

\newpage

\bibliographystyle{siam}
\bibliography{References/biblio}

\end{document}